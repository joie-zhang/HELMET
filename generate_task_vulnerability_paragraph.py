#!/usr/bin/env python3
"""
Generate a polished LaTeX paragraph about task vulnerability patterns
in the style of the provided reference paragraph.
"""

import pandas as pd
import numpy as np
import os

# Define paths
results_dir = '/scratch/gpfs/DANQIC/jz4391/HELMET/results'

# Load data
df_grouped = pd.read_csv(os.path.join(results_dir, 'task_deltas_data.csv'))
df_detailed = pd.read_csv(os.path.join(results_dir, 'task_deltas_data_detailed.csv'))

# Get task groups
task_groups = [col for col in df_grouped.columns if col != 'Technique']

# Group techniques into classes
quantization_techs = ['NF4', 'Int8']
eviction_techs = ['SnapKV', 'PyramidKV', 'StreamingLLM']
duoattn_tech = ['DuoAttention']

def get_technique_class_stats(tech_list, task):
    """Get mean and std for a technique class on a specific task"""
    values = []
    for tech in tech_list:
        if tech in df_grouped['Technique'].values:
            val = df_grouped[df_grouped['Technique'] == tech][task].values[0]
            if not np.isnan(val):
                values.append(val)
    if values:
        return np.mean(values), np.std(values)
    return np.nan, np.nan

# Calculate stats for each task by technique class
print("Calculating task vulnerability statistics...")

# Identify most and least vulnerable tasks
task_vulnerabilities = {}
for task in task_groups:
    values = df_grouped[task].values.astype(float)
    valid_values = values[~np.isnan(values)]
    if len(valid_values) > 0:
        task_vulnerabilities[task] = {
            'mean': np.mean(valid_values),
            'std': np.std(valid_values),
            'abs_mean': np.mean(np.abs(valid_values))
        }

# Sort by absolute mean to find most vulnerable
sorted_tasks = sorted(task_vulnerabilities.items(), key=lambda x: x[1]['abs_mean'], reverse=True)
most_vulnerable = sorted_tasks[:3]  # Top 3 most vulnerable
least_vulnerable = sorted_tasks[-3:]  # Top 3 least vulnerable

# Get specific stats for token eviction methods on vulnerable tasks
recall_eviction_mean, recall_eviction_std = get_technique_class_stats(eviction_techs, 'Recall')
rerank_eviction_mean, rerank_eviction_std = get_technique_class_stats(eviction_techs, 'Re-rank')
niah_eviction_mean, niah_eviction_std = get_technique_class_stats(eviction_techs, 'NIAH')

# Get stats for quantization on these tasks
recall_quant_mean, recall_quant_std = get_technique_class_stats(quantization_techs, 'Recall')
rerank_quant_mean, rerank_quant_std = get_technique_class_stats(quantization_techs, 'Re-rank')
niah_quant_mean, niah_quant_std = get_technique_class_stats(quantization_techs, 'NIAH')

# Get stats for DuoAttention
recall_duo_mean, recall_duo_std = get_technique_class_stats(duoattn_tech, 'Recall')
rerank_duo_mean, rerank_duo_std = get_technique_class_stats(duoattn_tech, 'Re-rank')

# Get stats for robust tasks
summ_eviction_mean, summ_eviction_std = get_technique_class_stats(eviction_techs, 'Summ')
icl_eviction_mean, icl_eviction_std = get_technique_class_stats(eviction_techs, 'ICL')
pseudo_eviction_mean, pseudo_eviction_std = get_technique_class_stats(eviction_techs, 'Pseudo')

summ_quant_mean, summ_quant_std = get_technique_class_stats(quantization_techs, 'Summ')
icl_quant_mean, icl_quant_std = get_technique_class_stats(quantization_techs, 'ICL')

# Calculate gaps between technique classes for vulnerable tasks
recall_gap = abs(recall_eviction_mean - recall_quant_mean)
rerank_gap = abs(rerank_eviction_mean - rerank_quant_mean)
niah_gap = abs(niah_eviction_mean - niah_quant_mean)

# Get overall averages by technique class
quant_all_tasks = []
eviction_all_tasks = []
for task in task_groups:
    qm, _ = get_technique_class_stats(quantization_techs, task)
    em, _ = get_technique_class_stats(eviction_techs, task)
    if not np.isnan(qm):
        quant_all_tasks.append(qm)
    if not np.isnan(em):
        eviction_all_tasks.append(em)

quant_overall_mean = np.mean(quant_all_tasks)
quant_overall_std = np.std(quant_all_tasks)
eviction_overall_mean = np.mean(eviction_all_tasks)
eviction_overall_std = np.std(eviction_all_tasks)

# Calculate task type categories
memory_retrieval_tasks = ['NIAH', 'Recall']
ir_tasks = ['RAG', 'Re-rank']
reasoning_tasks = ['Summ', 'Pseudo', 'ICL']

memory_eviction_values = []
ir_eviction_values = []
reasoning_eviction_values = []

for task in memory_retrieval_tasks:
    m, _ = get_technique_class_stats(eviction_techs, task)
    if not np.isnan(m):
        memory_eviction_values.append(m)

for task in ir_tasks:
    m, _ = get_technique_class_stats(eviction_techs, task)
    if not np.isnan(m):
        ir_eviction_values.append(m)

for task in reasoning_tasks:
    m, _ = get_technique_class_stats(eviction_techs, task)
    if not np.isnan(m):
        reasoning_eviction_values.append(m)

memory_eviction_mean = np.mean(memory_eviction_values)
ir_eviction_mean = np.mean(ir_eviction_values)
reasoning_eviction_mean = np.mean(reasoning_eviction_values)

# Generate paragraph
paragraph = f"""
In \\Cref{{fig:task_deltas}}, we analyze task-wise performance degradation across six efficient inference techniques, revealing striking differences in task vulnerability. Quantization methods (NF4 and Int8) demonstrate minimal and uniform degradation across all task categories, averaging {quant_overall_mean:.2f}\\% ± {quant_overall_std:.2f}\\%, with no task exceeding {max([abs(v) for v in quant_all_tasks]):.2f}\\% degradation. In stark contrast, token eviction methods (SnapKV, PyramidKV, StreamingLLM) show severe task-dependent performance degradation averaging {eviction_overall_mean:.2f}\\% ± {eviction_overall_std:.2f}\\%, with catastrophic failures on memory-intensive tasks. Specifically, Recall tasks suffer {recall_eviction_mean:.2f}\\% ± {recall_eviction_std:.2f}\\% degradation under token eviction versus {recall_quant_mean:.2f}\\% ± {recall_quant_std:.2f}\\% under quantization (gap: {recall_gap:.2f} percentage points), while Re-rank tasks degrade by {rerank_eviction_mean:.2f}\\% ± {rerank_eviction_std:.2f}\\% versus {rerank_quant_mean:.2f}\\% ± {rerank_quant_std:.2f}\\% (gap: {rerank_gap:.2f} percentage points). NIAH tasks similarly show {niah_eviction_mean:.2f}\\% ± {niah_eviction_std:.2f}\\% degradation under eviction versus {niah_quant_mean:.2f}\\% ± {niah_quant_std:.2f}\\% under quantization. Task type analysis reveals systematic patterns: memory retrieval tasks (NIAH, Recall) average {memory_eviction_mean:.2f}\\% degradation under eviction, information retrieval tasks (RAG, Re-rank) average {ir_eviction_mean:.2f}\\%, while reasoning tasks (Summ, Pseudo, ICL) demonstrate relative resilience at {reasoning_eviction_mean:.2f}\\%. DuoAttention shows moderate task-dependent behavior with {recall_duo_mean:.2f}\\% degradation on Recall and {rerank_duo_mean:.2f}\\% on Re-rank, substantially outperforming token eviction methods but exhibiting more task variance than quantization. These findings demonstrate that token eviction methods fundamentally compromise tasks requiring precise retrieval of cached information, while quantization preserves performance uniformly across task types, and reasoning tasks requiring semantic understanding remain relatively robust regardless of efficiency technique.
"""

# Print results
print("\n" + "=" * 100)
print("TASK VULNERABILITY PARAGRAPH")
print("=" * 100)
print(paragraph)

# Save to file
output_path = os.path.join(results_dir, 'task_vulnerability_paragraph.txt')
with open(output_path, 'w') as f:
    f.write(paragraph)

print(f"\n\nSaved paragraph to: {output_path}")

# Also print key statistics used
print("\n" + "=" * 100)
print("KEY STATISTICS SUMMARY")
print("=" * 100)

print("\nOVERALL TECHNIQUE CLASS PERFORMANCE:")
print(f"  Quantization: {quant_overall_mean:.2f}% ± {quant_overall_std:.2f}%")
print(f"  Token Eviction: {eviction_overall_mean:.2f}% ± {eviction_overall_std:.2f}%")

print("\nMOST VULNERABLE TASKS (Recall):")
print(f"  Token Eviction: {recall_eviction_mean:.2f}% ± {recall_eviction_std:.2f}%")
print(f"  Quantization: {recall_quant_mean:.2f}% ± {recall_quant_std:.2f}%")
print(f"  DuoAttention: {recall_duo_mean:.2f}%")
print(f"  Gap (Eviction vs Quant): {recall_gap:.2f} pp")

print("\nRE-RANK TASKS:")
print(f"  Token Eviction: {rerank_eviction_mean:.2f}% ± {rerank_eviction_std:.2f}%")
print(f"  Quantization: {rerank_quant_mean:.2f}% ± {rerank_quant_std:.2f}%")
print(f"  DuoAttention: {rerank_duo_mean:.2f}%")
print(f"  Gap (Eviction vs Quant): {rerank_gap:.2f} pp")

print("\nNIAH TASKS:")
print(f"  Token Eviction: {niah_eviction_mean:.2f}% ± {niah_eviction_std:.2f}%")
print(f"  Quantization: {niah_quant_mean:.2f}% ± {niah_quant_std:.2f}%")
print(f"  Gap (Eviction vs Quant): {niah_gap:.2f} pp")

print("\nTASK TYPE CATEGORIES (Token Eviction):")
print(f"  Memory Retrieval (NIAH, Recall): {memory_eviction_mean:.2f}%")
print(f"  Information Retrieval (RAG, Re-rank): {ir_eviction_mean:.2f}%")
print(f"  Reasoning (Summ, Pseudo, ICL): {reasoning_eviction_mean:.2f}%")

# Save stats to CSV
stats_data = {
    'Category': ['Quantization', 'Token Eviction', 'Recall-Eviction', 'Recall-Quant', 'Recall-Duo',
                 'Rerank-Eviction', 'Rerank-Quant', 'Rerank-Duo', 'NIAH-Eviction', 'NIAH-Quant',
                 'MemoryRetrieval-Eviction', 'InfoRetrieval-Eviction', 'Reasoning-Eviction'],
    'Mean': [quant_overall_mean, eviction_overall_mean, recall_eviction_mean, recall_quant_mean, recall_duo_mean,
             rerank_eviction_mean, rerank_quant_mean, rerank_duo_mean, niah_eviction_mean, niah_quant_mean,
             memory_eviction_mean, ir_eviction_mean, reasoning_eviction_mean],
    'Std': [quant_overall_std, eviction_overall_std, recall_eviction_std, recall_quant_std, np.nan,
            rerank_eviction_std, rerank_quant_std, np.nan, niah_eviction_std, niah_quant_std,
            np.nan, np.nan, np.nan]
}

df_stats = pd.DataFrame(stats_data)
stats_path = os.path.join(results_dir, 'task_vulnerability_paragraph_stats.csv')
df_stats.to_csv(stats_path, index=False)
print(f"\n\nSaved statistics to: {stats_path}")
