
In Figure [task_deltas], we analyze the task-wise performance degradation of six efficient inference techniques relative to the baseline, averaged across all models and context lengths. Quantization methods demonstrate minimal performance impact, with NF4 averaging -1.38% ± 1.58% degradation and Int8 showing -0.04% ± 1.19% degradation across task categories. In stark contrast, token eviction methods exhibit severe and inconsistent performance penalties: SnapKV (-11.58% ± 15.76%), PyramidKV (-12.11% ± 13.93%), and StreamingLLM (-22.18% ± 22.25%). The aggregate degradation for token eviction methods is -15.29% ± 18.34%, representing a 14.58 percentage point gap compared to quantization methods. Notably, recall-based tasks (Recall: -56.24%) and retrieval tasks (Re-rank: -27.78%) suffer disproportionately under token eviction, while reasoning tasks (Summ, ICL) show relative resilience. DuoAttention demonstrates competitive performance with -2.97% ± 3.77% average degradation, showing 2 out of 9 task categories with improvements or minimal degradation, though it shows vulnerabilities in specific tasks such as citation tasks and long-context generation tasks. These results highlight the critical importance of task-aware method selection, as token eviction methods systematically underperform on memory-intensive tasks while quantization provides more uniform preservation of capabilities.
