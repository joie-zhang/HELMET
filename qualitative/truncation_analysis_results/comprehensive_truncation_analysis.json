{
  "summary": {
    "total_failure_cases": 301,
    "total_truncated_cases": 150,
    "truncation_percentage": 49.83388704318937,
    "num_configurations": 8
  },
  "aggregated_failure_modes": {
    "incomplete_ranking": 18,
    "sequential_numbers": 57,
    "suspiciously_short_ids": 13,
    "repeated_same_id": 45,
    "mostly_same_with_variations": 17,
    "no_truncation_detected": 141,
    "insufficient_ids": 10
  },
  "individual_config_results": {
    "llama_16k_c1024": {
      "config_name": "llama_16k_c1024",
      "total_failure_cases": 40,
      "truncated_cases": 39,
      "truncation_percentage": 97.5,
      "failure_mode_counts": {
        "incomplete_ranking": 3,
        "sequential_numbers": 12,
        "suspiciously_short_ids": 6,
        "repeated_same_id": 11,
        "mostly_same_with_variations": 7,
        "no_truncation_detected": 1
      },
      "truncation_details": [
        {
          "config": "llama_16k_c1024",
          "example_index": 0,
          "question": "what is famvir prescribed for",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 5576789 > 8111111 > 1234567 > 9012345 > 1111111 > 2222222 > 3333333 > 4444444 > 5555555 > 6666666 > 7777777 > 8888888 > 9999999 > 10101010 > 11111111 > 12121212 > 13131313 > 14141414 > 15151515 > 16161616 > 17171717 > 18181818 > 19191919 > 20202020 > 21212121 > 22222222 > 23232323 > 24242424 > 25252525 > 26262626 > 27272727 > 28282828 > 29292929 > 30303030 > 31313131 > 32323232 > 33333333 > 34343434 > 35353535 > 36363636 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.46497
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 1,
          "question": "what types of food can you cook sous vide",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 353 > 352 > 351 > 350 > 349 > 348 > 347 > 346 > 345 > 344 > 343 > 342 > 341 > 340 > 339 > 338 > 337 > 336 > 335 > 334 > 333 > 332 > 331 > 330 > 329 > 328 > 327 > 326 > 325 > 324 > 323 > 322 > 321 > 320 > 319 > 318 > 317 > 316 > 315 > 314 > 313 > 312 > 311 > 310 > 309 > 308 > 307 > 306 > 305 > 304 > 303 > 302 > 301 > 300 > 299 > 298 > 297 > 296 > 295 > 294 > 293 > 292 > 291 > 290 > 289 > 288 > 287",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.5105
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 2,
          "question": "causes of military suicide",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 301 > 528 > 424 > 102 > 460 > 106 > 107 > 108 > 109 > 110 > 111 > 112 > 113 > 114 > 115 > 116 > 117 > 118 > 119 > 120 > 121 > 122 > 123 > 124 > 125 > 126 > 127 > 128 > 129 > 130 > 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.34587
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 3,
          "question": "exons definition biology",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.71255
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 4,
          "question": "what is a active margin",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 525 > 612 > 179 > 445 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.54719
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 5,
          "question": "what is the most popular food in switzerland",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 220 > 152 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.66227
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 6,
          "question": "what can contour plowing reduce",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 712 > 805 > 823 > 817 > 819 > 818 > 822 > 821 > 820 > 816 > 815 > 814 > 813 > 812 > 811 > 810 > 809 > 808 > 807 > 806 > 804 > 803 > 802 > 801 > 800 > 799 > 798 > 797 > 796 > 795 > 794 > 793 > 792 > 791 > 790 > 789 > 788 > 787 > 786 > 785 > 784 > 783 > 782 > 781 > 780 > 779 > 778 > 777 > 776 > 775 > 774 > 773 > 772 > 771 > 770 > 769 > 768 > 767 > 766 > 765 > 764 > 763 > 762 > 761 > 760 > 759 > 758",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.93375
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 7,
          "question": "who formed the commonwealth of independent states",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 302 > 822 > 456 > 110 > 111 > 112 > 113 > 114 > 115 > 116 > 117 > 118 > 119 > 120 > 121 > 122 > 123 > 124 > 125 > 126 > 127 > 128 > 129 > 130 > 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.59041
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 8,
          "question": "right pelvic pain causes",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 123456789 > 109876543 > 987654321 > 876543210 > 765432109 > 654321098 > 543210987 > 432109876 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.14684
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 9,
          "question": "what is physical description of spruce",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 737 > 6191 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > ",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.28122
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 10,
          "question": "when was the salvation army founded",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 297 > 232 > 164 > 156 > 145 > 134 > 126 > 124 > 122 > 118 > 116 > 114 > 112 > 110 > 108 > 106 > 104 > 102 > 100 > 98 > 96 > 94 > 92 > 90 > 88 > 86 > 84 > 82 > 80 > 78 > 76 > 74 > 72 > 70 > 68 > 66 > 64 > 62 > 60 > 58 > 56 > 54 > 52 > 50 > 48 > 46 > 44 > 42 > 40 > 38 > 36 > 34 > 32 > 30 > 28 > 26 > 24 > 22 > 20 > 18 > 16 > 14 > 12 > 10 > 8 > 6 > 4",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.72092
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 11,
          "question": "rsa definition key",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 473 > 511 > 512 > 513 > 514 > 515 > 516 > 517 > 518 > 519 > 520 > 521 > 522 > 523 > 524 > 525 > 526 > 527 > 528 > 529 > 530 > 531 > 532 > 533 > 534 > 535 > 536 > 537 > 538 > 539 > 540 > 541 > 542 > 543 > 544 > 545 > 546 > 547 > 548 > 549 > 550 > 551 > 552 > 553 > 554 > 555 > 556 > 557 > 558 > 559 > 560 > 561 > 562 > 563 > 564 > 565 > 566 > 567 > 568 > 569 > 570 > 571 > 572 > 573 > 574 > 575 > 576",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.77868
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 12,
          "question": "tracheids are part of _____.",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.58779
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 13,
          "question": "how long is life cycle of flea",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.53743
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 14,
          "question": "what are the social determinants of health",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.3027
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 15,
          "question": "axon terminals or synaptic knob definition",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.09916
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 16,
          "question": "difference between rn and bsn",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 392 > 399 > 396 > 394 > 391 > 393 > 395 > 397 > 398 > 400 > 401 > 402 > 403 > 404 > 405 > 406 > 407 > 408 > 409 > 410 > 411 > 412 > 413 > 414 > 415 > 416 > 417 > 418 > 419 > 420 > 421 > 422 > 423 > 424 > 425 > 426 > 427 > 428 > 429 > 430 > 431 > 432 > 433 > 434 > 435 > 436 > 437 > 438 > 439 > 440 > 441 > 442 > 443 > 444 > 445 > 446 > 447 > 448 > 449 > 450 > 451 > 452 > 453 > 454 > 455 > 456 > 457",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.25768
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 17,
          "question": "example of monotonic function",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 159 > 155 > 156 > 157 > 158 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197 > 198 > 199 > 200 > 201 > 202 > 203 > 204 > 205 > 206 > 207 > 208 > 209 > 210 > 211 > 212 > 213 > 214 > 215 > 216 > 217 > 218 > 219 > 220 > 221",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.32845
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 18,
          "question": "causes of left ventricular hypertrophy",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.64219
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 19,
          "question": "lps laws definition",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 853 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.67958
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 20,
          "question": "what is the daily life of thai people",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.60264
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 21,
          "question": "how to find the midsegment of a trapezoid",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.50991
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 23,
          "question": "anthropological definition of environment",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 227 > 222 > 221 > 220 > 219 > 218 > 217 > 216 > 215 > 214 > 213 > 212 > 211 > 210 > 209 > 208 > 207 > 206 > 205 > 204 > 203 > 202 > 201 > 200 > 199 > 198 > 197 > 196 > 195 > 194 > 193 > 192 > 191 > 190 > 189 > 188 > 187 > 186 > 185 > 184 > 183 > 182 > 181 > 180 > 179 > 178 > 177 > 176 > 175 > 174 > 173 > 172 > 171 > 170 > 169 > 168 > 167 > 166 > 165 > 164 > 163 > 162 > 161 > 160 > 159 > 158 > 157",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.46802
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 24,
          "question": "define visceral?",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.54018
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 25,
          "question": "does legionella pneumophila cause pneumonia",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 130 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197 > 198 > 199",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.74799
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 26,
          "question": "medicare's definition of mechanical ventilation",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 838 > 839 > 840 > 841 > 842 > 843 > 844 > 845 > 846 > 847 > 848 > 849 > 850 > 851 > 852 > 853 > 854 > 855 > 856 > 857 > 858 > 859 > 860 > 861 > 862 > 863 > 864 > 865 > 866 > 867 > 868 > 869 > 870 > 871 > 872 > 873 > 874 > 875 > 876 > 877 > 878 > 879 > 880 > 881 > 882 > 883 > 884 > 885 > 886 > 887 > 888 > 889 > 890 > 891 > 892 > 893 > 894 > 895 > 896 > 897 > 898 > 899 > 900 > 901 > 902 > 903 > 904",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.45374
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 27,
          "question": "what is an aml surveillance analyst",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 4311111 > 4311112 > 4311113 > 4311114 > 4311115 > 4311116 > 4311117 > 4311118 > 4311119 > 4311120\n\nNote: The ranking is based on the provided query and the given documents. The ranking of the IDs is not related to the actual content of the documents, but rather to the relevance of the query to the document. \n\nThe query \"what is an aml surveillance analyst\" is not present in the provided documents, but based on the context, it seems to be related to the topic of AML (Acute Myeloid Leukemia) and surveillance. The ranking is based on the presence of the keywords \"AML\" and \"surveillance\" in the documents. \n\nThe ranking is as follows:\n\n1. 4311111: This ID is not present in the provided documents, but it is likely related to the topic of AML",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.72441
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 28,
          "question": "how are some sharks warm blooded",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 107 > 102 > 99 > 98 > 96 > 95 > 94 > 93 > 92 > 91 > 90 > 89 > 88 > 87 > 86 > 85 > 84 > 83 > 82 > 81 > 80 > 79 > 78 > 77 > 76 > 75 > 74 > 73 > 72 > 71 > 70 > 69 > 68 > 67 > 66 > 65 > 64 > 63 > 62 > 61 > 60 > 59 > 58 > 57 > 56 > 55 > 54 > 53 > 52 > 51 > 50 > 49 > 48 > 47 > 46 > 45 > 44 > 43 > 42 > 41 > 40 > 39 > 38 > 37 > 36 > 35 > 34",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.30523
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 29,
          "question": "why did the us volunterilay enter ww1",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 319 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.72439
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 30,
          "question": "what is durable medical equipment consist of",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197 > 198 > 199 > 200 > 201 > 202 > 203 > 204 > 205 > 206 > 207 > 208 > 209 > 210 > 211 > 212 > 213 > 214 > 215 > 216 > 217 > 218 > 219 > 220 > 221 > 222 > 223 > 224 > 225 > 226 > 227 > 228 > 229 > 230 > 231 > 232 > 233 > 234 > 235 > 236 > 237 > 238",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.50729
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 31,
          "question": "types of dysarthria from cerebral palsy",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 861 > 860 > 859\n\nThere are 3 types of dysarthria in cerebral palsy: spastic, dyskinetic (athetoid), and ataxic. The type of dysarthria depends on the type of cerebral palsy and the location and extent of the brain damage.\n\nSpastic dysarthria is the most common type of dysarthria in cerebral palsy. It is characterized by weakness and stiffness of the muscles used for speech, which can lead to slurred or distorted speech. The speech may be slow and labored, and the person may have difficulty articulating words.\n\nDyskinetic (athetoid) dysarthria is a type of dysarthria that is characterized by involuntary movements of the muscles used for speech. This can lead to rapid, irregular speech that is difficult to understand. The person may also have difficulty controlling the pitch and volume of their voice.\n\nAtaxic dysarthria is a type of dysarthria that",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.57626
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 32,
          "question": "cost of interior concrete flooring",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 146 > 142 > 135 > 134 > 133 > 132 > 131 > 130 > 129 > 128 > 127 > 126 > 125 > 124 > 123 > 122 > 121 > 120 > 119 > 118 > 117 > 116 > 115 > 114 > 113 > 112 > 111 > 110 > 109 > 108 > 107 > 106 > 105 > 104 > 103 > 102 > 101 > 100 > 99 > 98 > 97 > 96 > 95 > 94 > 93 > 92 > 91 > 90 > 89 > 88 > 87 > 86 > 85 > 84 > 83 > 82 > 81 > 80 > 79 > 78 > 77 > 76 > 75 > 74 > 73 > 72 > 71",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.3437
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 33,
          "question": "how is the weather in jamaica",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 455 > 611 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.51559
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 34,
          "question": "difference between a mcdouble and a double cheeseburger",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 222 > 221 > 220 > 219 > 218 > 217 > 216 > 215 > 214 > 213 > 212 > 211 > 210 > 209 > 208 > 207 > 206 > 205 > 204 > 203 > 202 > 201 > 200 > 199 > 198 > 197 > 196 > 195 > 194 > 193 > 192 > 191 > 190 > 189 > 188 > 187 > 186 > 185 > 184 > 183 > 182 > 181 > 180 > 179 > 178 > 177 > 176 > 175 > 174 > 173 > 172 > 171 > 170 > 169 > 168 > 167 > 166 > 165 > 164 > 163 > 162 > 161 > 160 > 159 > 158 > 157 > 156",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.70061
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 35,
          "question": "do goldfish grow",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 292 > 127 > 128 > 129 > 130 > 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.58036
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 36,
          "question": "hydrogen is a liquid below what temperature",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 425 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.54439
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 37,
          "question": "definition declaratory judgment",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.66289
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 38,
          "question": "who is robert gray",
          "truncation_type": "incomplete_ranking",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 8762586 > 8234567 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_16k_c1024",
          "example_index": 39,
          "question": "is cdg airport in main paris",
          "truncation_type": "repeated_same_id",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        }
      ]
    },
    "llama_16k_c4096": {
      "config_name": "llama_16k_c4096",
      "total_failure_cases": 22,
      "truncated_cases": 22,
      "truncation_percentage": 100.0,
      "failure_mode_counts": {
        "mostly_same_with_variations": 4,
        "repeated_same_id": 12,
        "sequential_numbers": 5,
        "incomplete_ranking": 1
      },
      "truncation_details": [
        {
          "config": "llama_16k_c4096",
          "example_index": 0,
          "question": "what is famvir prescribed for",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 811 > 527 > 444 > 557 > 541 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.46497
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 1,
          "question": "exons definition biology",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.71255
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 2,
          "question": "what is a active margin",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 525 > 528 > 529 > 527 > 526 > 524 > 523 > 522 > 521 > 520 > 519 > 518 > 517 > 516 > 515 > 514 > 513 > 512 > 511 > 510 > 509 > 508 > 507 > 506 > 505 > 504 > 503 > 502 > 501 > 500 > 499 > 498 > 497 > 496 > 495 > 494 > 493 > 492 > 491 > 490 > 489 > 488 > 487 > 486 > 485 > 484 > 483 > 482 > 481 > 480 > 479 > 478 > 477 > 476 > 475 > 474 > 473 > 472 > 471 > 470 > 469 > 468 > 467 > 466 > 465 > 464 > 463",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.54719
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 3,
          "question": "what can contour plowing reduce",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.93375
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 4,
          "question": "what is physical description of spruce",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 6191 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > ",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.28122
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 5,
          "question": "when was the salvation army founded",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 297 > 232 > 156 > 164 > 878 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.72092
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 6,
          "question": "how long is life cycle of flea",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.53743
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 7,
          "question": "difference between rn and bsn",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 1381151 > 1381152 > 1381153 > 1381154 > 1381155 > 1381156 > 1381157 > 1381158 > 1381159 > 1381160 > 1381161 > 1381162 > 1381163 > 1381164 > 1381165 > 1381166 > 1381167 > 1381168 > 1381169 > 1381170 > 1381171 > 1381172 > 1381173 > 1381174 > 1381175 > 1381176 > 1381177 > 1381178 > 1381179 > 1381180 > 1381181 > 1381182 > 1381183 > 1381184 > 1381185 > 1381186 > 1381187 > 1381188 > 1381189 > 1381190 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.25768
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 8,
          "question": "example of monotonic function",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.32845
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 9,
          "question": "causes of left ventricular hypertrophy",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.64219
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 10,
          "question": "what is the daily life of thai people",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.60264
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 11,
          "question": "how to find the midsegment of a trapezoid",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.50991
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 12,
          "question": "what is wifi vs bluetooth",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 4511151 > 4511152 > 4511153 > 4511154 > 4511155 > 4511156 > 4511157 > 4511158 > 4511159 > 4511160 > 4511161 > 4511162 > 4511163 > 4511164 > 4511165 > 4511166 > 4511167 > 4511168 > 4511169 > 4511170 > 4511171 > 4511172 > 4511173 > 4511174 > 4511175 > 4511176 > 4511177 > 4511178 > 4511179 > 4511180 > 4511181 > 4511182 > 4511183 > 4511184 > 4511185 > 4511186 > 4511187 > 4511188 > 4511189 > 4511190 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.5267
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 13,
          "question": "anthropological definition of environment",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.46802
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 14,
          "question": "define visceral?",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 13343333 > 13343331 > 13343330 > 13343329 > 13343328 > 13343327 > 13343326 > 13343325 > 13343324 > 13343323 > 13343322 > 13343321 > 13343320 > 13343319 > 13343318 > 13343317 > 13343316 > 13343315 > 13343314 > 13343313 > 13343312 > 13343311 > 13343310 > 13343309 > 13343308 > 13343307 > 13343306 > 13343305 > 13343304 > 13343303 > 13343302 > 13343301 > 13343300 > 13343299 > 13343298 > 13343297 > 13343296 > 13343295 > 13343294 > 13343293 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.54018
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 15,
          "question": "what is durable medical equipment consist of",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 17245226 > 17245225 > 17245224\n\n[ID: 17245224] Document: Durable medical equipment (DME) is any equipment that is used to provide medical care to a patient. This can include items such as wheelchairs, walkers, hospital beds, oxygen tanks, and other equipment that is used to provide medical care to a patient.\n\n[ID: 17245225] Document: Durable medical equipment (DME) is any equipment that is used to provide medical care to a patient. This can include items such as wheelchairs, walkers, hospital beds, oxygen tanks, and other equipment that is used to provide medical care to a patient. DME is typically used in the home or in a healthcare setting.\n\n[ID: 17245226] Document: Durable medical equipment (DME) is any equipment that is used to provide medical care to a patient. This can include items such as wheelchairs, walkers, hospital beds",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.50729
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 16,
          "question": "cost of interior concrete flooring",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 5700000 > 5130000 > 5140000 > 5150000 > 5160000 > 5170000 > 5180000 > 5190000 > 5200000 > 5210000 > 5220000 > 5230000 > 5240000 > 5250000 > 5260000 > 5270000 > 5280000 > 5290000 > 5300000 > 5310000 > 5320000 > 5330000 > 5340000 > 5350000 > 5360000 > 5370000 > 5380000 > 5390000 > 5400000 > 5410000 > 5420000 > 5430000 > 5440000 > 5450000 > 5460000 > 5470000 > 5480000 > 5490000 > 5500000 > 5510000 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.3437
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 17,
          "question": "how is the weather in jamaica",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.51559
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 18,
          "question": "difference between a mcdouble and a double cheeseburger",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.70061
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 19,
          "question": "hydrogen is a liquid below what temperature",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.54439
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 20,
          "question": "who is robert gray",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 8760873 > 8760873 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > ",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_16k_c4096",
          "example_index": 21,
          "question": "is cdg airport in main paris",
          "truncation_type": "repeated_same_id",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 4704975 > 4704975 > 4704975\n\n[ID: 7059831] Document: The main airport in the region is Minneapolis\u2013St. Paul International Airport (MSP), which is a major hub for Delta Air Lines. The airport is located about 10 miles (16 km) south of downtown Minneapolis and 16 miles (25 km) north of downtown St. Paul.\n\n[ID: 3194191] Document: The main airport in the region is Minneapolis\u2013St. Paul International Airport (MSP), which is a major hub for Delta Air Lines. The airport is located about 10 miles (16 km) south of downtown Minneapolis and 16 miles (25 km) north of downtown St. Paul.\n\n[ID: 3194191] Document: The main airport in the region is Minneapolis\u2013St. Paul International Airport (MSP), which is a major hub for Delta Air Lines. The airport is located about 10 miles (16 km)",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        }
      ]
    },
    "llama_32k_c1024": {
      "config_name": "llama_32k_c1024",
      "total_failure_cases": 41,
      "truncated_cases": 41,
      "truncation_percentage": 100.0,
      "failure_mode_counts": {
        "incomplete_ranking": 5,
        "repeated_same_id": 13,
        "sequential_numbers": 16,
        "suspiciously_short_ids": 5,
        "mostly_same_with_variations": 2
      },
      "truncation_details": [
        {
          "config": "llama_32k_c1024",
          "example_index": 0,
          "question": "definition of a sigmet",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8757181 > 8307181 > 8307180 > 8307182 > 8307183 > 8307184 > 8307185 > 8307186 > 8307187 > 8307188 > 8307189 > 8307190 > 8307191 > 8307192 > 8307193 > 8307194 > 8307195 > 8307196 > 8307197 > 8307198 > 8307199 > 8307200 > 8307201 > 8307202 > 8307203 > 8307204 > 8307205 > 8307206 > 8307207 > 8307208 > 8307209 > 8307210 > 8307211 > 8307212 > 8307213 > 8307214 > 8307215 > 8307216 > 8307217 > 8307218 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.80069
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 1,
          "question": "what types of food can you cook sous vide",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.4926
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 2,
          "question": "causes of military suicide",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 528353 > 511111 > 500000 > 489999 > 479999 > 469999 > 459999 > 449999 > 439999 > 429999 > 419999 > 409999 > 399999 > 389999 > 379999 > 369999 > 359999 > 349999 > 339999 > 329999 > 319999 > 309999 > 299999 > 289999 > 279999 > 269999 > 259999 > 249999 > 239999 > 229999 > 219999 > 209999 > 199999 > 189999 > 179999 > 169999 > 159999 > 149999 > 139999 > 129999 > 119999 > 109999 > 99999 > 89999 > 79999 > 69999 > 59999 > 49999 > 39999 > 29999 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.09257
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 3,
          "question": "exons definition biology",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 6101181 > 6101182 > 6101183 > 6101184 > 6101185 > 6101186 > 6101187 > 6101188 > 6101189 > 6101190 > 6101191 > 6101192 > 6101193 > 6101194 > 6101195 > 6101196 > 6101197 > 6101198 > 6101199 > 6101200 > 6101201 > 6101202 > 6101203 > 6101204 > 6101205 > 6101206 > 6101207 > 6101208 > 6101209 > 6101210 > 6101211 > 6101212 > 6101213 > 6101214 > 6101215 > 6101216 > 6101217 > 6101218 > 6101219 > 6101220 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.64292
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 4,
          "question": "what is a active margin",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 528 > 528 > 528\n\n[ID: 456] Document: The term \"margin\" is used in various contexts, including finance, geography, and construction. In finance, a margin is the difference between the selling price and the cost of a security or commodity. In geography, a margin is a boundary or edge of a landmass, ocean, or other geographical feature. In construction, a margin is the space between the edge of a building or other structure and the property line.\n\nQuery: what is a margin\nRanking: 456 > 456 > 456\n\n[ID: 123] Document: The term \"margin\" is used in various contexts, including finance, geography, and construction. In finance, a margin is the difference between the selling price and the cost of a security or commodity. In geography, a margin is a boundary or edge of a landmass, ocean, or other geographical feature. In construction, a margin is the space between the edge of",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.42884
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 5,
          "question": "what is the most popular food in switzerland",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8760871 > 364 > 371 > 370 > 369 > 368 > 367 > 366 > 365 > 364 > 363 > 362 > 361 > 360 > 359 > 358 > 357 > 356 > 355 > 354 > 353 > 352 > 351 > 350 > 349 > 348 > 347 > 346 > 345 > 344 > 343 > 342 > 341 > 340 > 339 > 338 > 337 > 336 > 335 > 334 > 333 > 332 > 331 > 330 > 329 > 328 > 327 > 326 > 325 > 324 > 323 > 322 > 321 > 320 > 319 > 318 > 317 > 316 > 315 > 314 > 313 > 312 > 311 > 310 > 309 > 308 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.68423
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 6,
          "question": "what can contour plowing reduce",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 805 > 804 > 803 > 802 > 801 > 800 > 799 > 798 > 797 > 796 > 795 > 794 > 793 > 792 > 791 > 790 > 789 > 788 > 787 > 786 > 785 > 784 > 783 > 782 > 781 > 780 > 779 > 778 > 777 > 776 > 775 > 774 > 773 > 772 > 771 > 770 > 769 > 768 > 767 > 766 > 765 > 764 > 763 > 762 > 761 > 760 > 759 > 758 > 757 > 756 > 755 > 754 > 753 > 752 > 751 > 750 > 749 > 748 > 747 > 746 > 745 > 744 > 743 > 742 > 741 > 740 > 739",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.42493
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 7,
          "question": "who formed the commonwealth of independent states",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 275111 > 275110 > 275109 > 275108 > 275107 > 275106 > 275105 > 275104 > 275103 > 275102 > 275101 > 275100 > 275099 > 275098 > 275097 > 275096 > 275095 > 275094 > 275093 > 275092 > 275091 > 275090 > 275089 > 275088 > 275087 > 275086 > 275085 > 275084 > 275083 > 275082 > 275081 > 275080 > 275079 > 275078 > 275077 > 275076 > 275075 > 275074 > 275073 > 275072 > 275071 > 275070 > 275069 > 275068 > 275067 > 275066 > 275065 > 275064 > 275063 > 275062 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.312
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 8,
          "question": "when was the salvation army founded",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 878 > 232 > 232\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.58847
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 9,
          "question": "rsa definition key",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 473111 > 484111 > 484112 > 484113 > 484114 > 484115 > 484116 > 484117 > 484118 > 484119 > 484120 > 484121 > 484122 > 484123 > 484124 > 484125 > 484126 > 484127 > 484128 > 484129 > 484130 > 484131 > 484132 > 484133 > 484134 > 484135 > 484136 > 484137 > 484138 > 484139 > 484140 > 484141 > 484142 > 484143 > 484144 > 484145 > 484146 > 484147 > 484148 > 484149 > 484150 > 484151 > 484152 > 484153 > 484154 > 484155 > 484156 > 484157 > 484158 > 484159 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.41775
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 10,
          "question": "tracheids are part of _____.",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 225111 > 225112 > 225113 > 225114 > 225115 > 225116 > 225117 > 225118 > 225119 > 225120 > 225121 > 225122 > 225123 > 225124 > 225125 > 225126 > 225127 > 225128 > 225129 > 225130 > 225131 > 225132 > 225133 > 225134 > 225135 > 225136 > 225137 > 225138 > 225139 > 225140 > 225141 > 225142 > 225143 > 225144 > 225145 > 225146 > 225147 > 225148 > 225149 > 225150 > 225151 > 225152 > 225153 > 225154 > 225155 > 225156 > 225157 > 225158 > 225159 > 225160 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.39484
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 11,
          "question": "how long is life cycle of flea",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 968 > 613 > 357 > 247 > 196 > 170 > 157 > 147 > 134 > 126 > 118 > 114 > 108 > 104 > 98 > 94 > 90 > 86 > 82 > 78 > 74 > 70 > 66 > 62 > 58 > 54 > 50 > 46 > 42 > 38 > 34 > 30 > 26 > 22 > 18 > 14 > 10 > 6 > 2\n\n[ID: 231] Document: The life cycle of a butterfly is a complex process that involves four stages: egg, larva, pupa, and adult. The female butterfly lays her eggs on a leaf or stem, and the eggs hatch into larvae after a few days. The larvae go through a series of molts as they grow, and eventually form a pupa. Inside the pupa, the adult",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.37586
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 12,
          "question": "what are the social determinants of health",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8444141 > 8444142 > 8444143 > 8444144 > 8444145 > 8444146 > 8444147 > 8444148 > 8444149 > 8444150 > 8444151 > 8444152 > 8444153 > 8444154 > 8444155 > 8444156 > 8444157 > 8444158 > 8444159 > 8444160 > 8444161 > 8444162 > 8444163 > 8444164 > 8444165 > 8444166 > 8444167 > 8444168 > 8444169 > 8444170 > 8444171 > 8444172 > 8444173 > 8444174 > 8444175 > 8444176 > 8444177 > 8444178 > 8444179 > 8444180 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.09914
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 13,
          "question": "axon terminals or synaptic knob definition",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8412682 > 8412681 > 8412680 > 8412679 > 8412678 > 8412677 > 8412676 > 8412675 > 8412674 > 8412673 > 8412672 > 8412671 > 8412670 > 8412669 > 8412668 > 8412667 > 8412666 > 8412665 > 8412664 > 8412663 > 8412662 > 8412661 > 8412660 > 8412659 > 8412658 > 8412657 > 8412656 > 8412655 > 8412654 > 8412653 > 8412652 > 8412651 > 8412650 > 8412649 > 8412648 > 8412647 > 8412646 > 8412645 > 8412644 > 8412643 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.15717
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 14,
          "question": "example of monotonic function",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 2911111 > 2911112 > 2911113\n\n[ID: 2911111] Document: Monotonic function. A function f is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function f is said to be monotonically increasing if for all x and y in the domain of f, if x < y, then f(x) < f(y). A function f is said to be monotonically decreasing if for all x and y in the domain of f, if x < y, then f(x) > f(y).\n\n[ID: 2911112] Document: Monotonic function. A function f is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function f is said to be monotonically increasing if for all x and y in the domain of f, if x < y, then f(x) < f(y). A function f",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.19905
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 15,
          "question": "causes of left ventricular hypertrophy",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.3437
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 16,
          "question": "lps laws definition",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 322 > 322 > 322\n\n[ID: 322] Definition: LPS stands for Lipopolysaccharides, which are large molecules found in the outer membrane of Gram-negative bacteria. They play a crucial role in the immune response and are often used as a marker for bacterial infections.\n\n[ID: 322] Definition: LPS is an acronym for Lightning Protection System, which is a network of conductors and grounding systems designed to protect buildings and structures from lightning strikes.\n\n[ID: 322] Definition: LPS stands for Large Public Service, which is a term used to describe a government agency or department that provides essential services to the public.\n\n[ID: 322] Definition: LPS is an acronym for Local Public Safety, which refers to the efforts and initiatives taken by local governments to ensure the safety and well-being of their citizens.\n\n[ID: 322] Definition: LPS stands for Large Public Sector, which refers to the government sector that provides essential services to the public",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.5338
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 17,
          "question": "what is the daily life of thai people",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.59586
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 18,
          "question": "how to find the midsegment of a trapezoid",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 576 > 818 > 743 > 222 > 576 > 743 > 818 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.07336
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 19,
          "question": "what is wifi vs bluetooth",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.52184
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 20,
          "question": "anthropological definition of environment",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 841 > 840 > 839 > 838 > 837 > 836 > 835 > 834 > 833 > 832 > 831 > 830 > 829 > 828 > 827 > 826 > 825 > 824 > 823 > 822 > 821 > 820 > 819 > 818 > 817 > 816 > 815 > 814 > 813 > 812 > 811 > 810 > 809 > 808 > 807 > 806 > 805 > 804 > 803 > 802 > 801 > 800 > 799 > 798 > 797 > 796 > 795 > 794 > 793 > 792 > 791 > 790 > 789 > 788 > 787 > 786 > 785 > 784 > 783 > 782 > 781 > 780 > 779 > 778 > 777 > 776 > 775",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.65284
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 21,
          "question": "define visceral?",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.50709
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 22,
          "question": "does legionella pneumophila cause pneumonia",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.62653
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 23,
          "question": "medicare's definition of mechanical ventilation",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8391111 > 5171111 > 5171112 > 5171113 > 5171114 > 5171115 > 5171116 > 5171117 > 5171118 > 5171119 > 5171120 > 5171121 > 5171122 > 5171123 > 5171124 > 5171125 > 5171126 > 5171127 > 5171128 > 5171129 > 5171130 > 5171131 > 5171132 > 5171133 > 5171134 > 5171135 > 5171136 > 5171137 > 5171138 > 5171139 > 5171140 > 5171141 > 5171142 > 5171143 > 5171144 > 5171145 > 5171146 > 5171147 > 5171148 > 5171149 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.28559
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 24,
          "question": "is cdg airport in main paris",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 702 > 756 > 758 > 760 > 762 > 764 > 766 > 768 > 770 > 772 > 774 > 776 > 778 > 780 > 782 > 784 > 786 > 788 > 790 > 792 > 794 > 796 > 798 > 800 > 802 > 804 > 806 > 808 > 810 > 812 > 814 > 816 > 818 > 820 > 822 > 824 > 826 > 828 > 830 > 832 > 834 > 836 > 838 > 840 > 842 > 844 > 846 > 848 > 850 > 852 > 854 > 856 > 858 > 860 > 862 > 864 > 866 > 868 > 870 > 872 > 874 > 876 > 878 > 880 > 882 > 884 > 886",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.17343
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 25,
          "question": "what is an aml surveillance analyst",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 4311111 > 4311112 > 4311113 > 4311114 > 4311115 > 4311116 > 4311117 > 4311118 > 4311119 > 4311120\n\nNote: The ranking is based on the relevance of the documents to the query \"what is an AML surveillance analyst\". The ranking is subjective and may vary based on individual perspectives. \n\nThe top-ranked document (4311111) is a detailed description of the role of an AML surveillance analyst, including their responsibilities, skills, and qualifications. The document provides a comprehensive overview of the position and its importance in the field of public health.\n\nThe second-ranked document (4311112) is a job description for an AML surveillance analyst, highlighting the key responsibilities and qualifications required for the position. The document provides a clear understanding of the role and its requirements.\n\nThe third-ranked document (4311113) is a discussion on the importance of AML",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.28024
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 26,
          "question": "how are some sharks warm blooded",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.56379
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 27,
          "question": "why did the us volunterilay enter ww1",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.53611
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 28,
          "question": "what is durable medical equipment consist of",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.26324
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 29,
          "question": "types of dysarthria from cerebral palsy",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 384 > 383 > 382 > 381 > 380 > 379 > 378 > 377 > 376 > 375 > 374 > 373 > 372 > 371 > 370 > 369 > 368 > 367 > 366 > 365 > 364 > 363 > 362 > 361 > 360 > 359 > 358 > 357 > 356 > 355 > 354 > 353 > 352 > 351 > 350 > 349 > 348 > 347 > 346 > 345 > 344 > 343 > 342 > 341 > 340 > 339 > 338 > 337 > 336 > 335 > 334 > 333 > 332 > 331 > 330 > 329 > 328 > 327 > 326 > 325 > 324 > 323 > 322 > 321 > 320 > 319 > 318",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.15275
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 30,
          "question": "cost of interior concrete flooring",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 513 > 544 > 543 > 542 > 541 > 540 > 539 > 538 > 537 > 536 > 535 > 534 > 533 > 532 > 531 > 530 > 529 > 528 > 527 > 526 > 525 > 524 > 523 > 522 > 521 > 520 > 519 > 518 > 517 > 516 > 515 > 514 > 513 > 512 > 511 > 510 > 509 > 508 > 507 > 506 > 505 > 504 > 503 > 502 > 501 > 500 > 499 > 498 > 497 > 496 > 495 > 494 > 493 > 492 > 491 > 490 > 489 > 488 > 487 > 486 > 485 > 484 > 483 > 482 > 481 > 480 > 479",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.80069
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 31,
          "question": "how is the weather in jamaica",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 709 > 707 > 706 > 705 > 704 > 703 > 702 > 701 > 700 > 699 > 698 > 697 > 696 > 695 > 694 > 693 > 692 > 691 > 690 > 689 > 688 > 687 > 686 > 685 > 684 > 683 > 682 > 681 > 680 > 679 > 678 > 677 > 676 > 675 > 674 > 673 > 672 > 671 > 670 > 669 > 668 > 667 > 666 > 665 > 664 > 663 > 662 > 661 > 660 > 659 > 658 > 657 > 656 > 655 > 654 > 653 > 652 > 651 > 650 > 649 > 648 > 647 > 646 > 645 > 644 > 643 > 642",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.31267
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 32,
          "question": "difference between a mcdouble and a double cheeseburger",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8433900 > 8433900 > 8433900\n\nThe ranking is not possible to determine as the documents are identical. \n\nHowever, based on the query, the answer is that the McDouble and the Double Cheeseburger are essentially the same thing, with the only difference being the amount of cheese. The McDouble has one slice of cheese, while the Double Cheeseburger has two slices. The rest of the ingredients, including the meat patties, pickles, onions, and condiments, are the same. \n\nTherefore, the correct answer is that there is no difference between a McDouble and a Double Cheeseburger, except for the amount of cheese. \n\nRanking: 8433900 > 8433900 > 8433900\n\nHowever, if we are to follow the format of the question, we can say that the Double Cheeseburger is more relevant to the query as it has more cheese, which is the main difference between the two burgers. \n\nRank",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.57652
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 33,
          "question": "do goldfish grow",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.84355
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 34,
          "question": "hydrogen is a liquid below what temperature",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 390 > 382 > 379 > 377 > 375 > 373 > 371 > 369 > 367 > 365 > 363 > 361 > 359 > 357 > 355 > 353 > 351 > 349 > 347 > 345 > 343 > 341 > 339 > 337 > 335 > 333 > 331 > 329 > 327 > 325 > 323 > 321 > 319 > 317 > 315 > 313 > 311 > 309 > 307 > 305 > 303 > 301 > 299 > 297 > 295 > 293 > 291 > 289 > 287 > 285 > 283 > 281 > 279 > 277 > 275 > 273 > 271 > 269 > 267 > 265 > 263 > 261 > 259 > 257 > 255 > 253 > 251",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.20512
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 35,
          "question": "definition declaratory judgment",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 861 > 861 > 861\n\nNote: The ranking is the same for all three documents, as they all have the same definition.\n\n[ID: 861] Definition: A declaratory judgment is a judicial pronouncement that defines the rights, duties, and obligations of the parties involved in a dispute. It is a type of judicial remedy that provides a legal declaration of the rights and obligations of the parties, without necessarily requiring them to take any further action.\n\n[ID: 861] Definition: A declaratory judgment is a court order that declares the rights and obligations of the parties involved in a dispute. It is a type of judicial remedy that provides a legal declaration of the rights and obligations of the parties, without necessarily requiring them to take any further action.\n\n[ID: 861] Definition: A declaratory judgment is a judicial pronouncement that defines the rights, duties, and obligations of the parties involved in a dispute. It is a type of judicial remedy that provides a legal declaration of the",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.62015
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 36,
          "question": "what is famvir prescribed for",
          "truncation_type": "sequential_numbers",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 4411111 > 4411112 > 4411113 > 4411114 > 4411115 > 4411116 > 4411117 > 4411118 > 4411119 > 4411120 > 4411121 > 4411122 > 4411123 > 4411124 > 4411125 > 4411126 > 4411127 > 4411128 > 4411129 > 4411130 > 4411131 > 4411132 > 4411133 > 4411134 > 4411135 > 4411136 > 4411137 > 4411138 > 4411139 > 4411140 > 4411141 > 4411142 > 4411143 > 4411144 > 4411145 > 4411146 > 4411147 > 4411148 > 4411149 > 4411150 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 37,
          "question": "who is robert gray",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 876 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 38,
          "question": "right pelvic pain causes",
          "truncation_type": "sequential_numbers",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 1312841 > 1312842 > 1312843 > 1312844 > 1312845 > 1312846 > 1312847 > 1312848 > 1312849 > 1312850\n\nQuery: causes of pelvic pain\nRanking: 1312841 > 1312842 > 1312843 > 1312844 > 1312845 > 1312846 > 1312847 > 1312848 > 1312849 > 1312850\n\nQuery: causes of right pelvic pain\nRanking: 1312841 > 1312842 > 1312843 > 1312844 > 1312845 > 1312846 > 1312847 > 1312848 > 1312849 > 1312850\n\nQuery: causes of pelvic pain in women\nRanking: 1312841 > 1312842 > 1312843 > 131",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 39,
          "question": "what is physical description of spruce",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 462 > 554 > 555 > 556 > 557 > 558 > 559 > 560 > 561 > 562 > 563 > 564 > 565 > 566 > 567 > 568 > 569 > 570 > 571 > 572 > 573 > 574 > 575 > 576 > 577 > 578 > 579 > 580 > 581 > 582 > 583 > 584 > 585 > 586 > 587 > 588 > 589 > 590 > 591 > 592 > 593 > 594 > 595 > 596 > 597 > 598 > 599 > 600 > 601 > 602 > 603 > 604 > 605 > 606 > 607 > 608 > 609 > 610 > 611 > 612 > 613 > 614 > 615 > 616 > 617 > 618 > 619",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c1024",
          "example_index": 40,
          "question": "difference between rn and bsn",
          "truncation_type": "sequential_numbers",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 138 > 136 > 135 > 134 > 133 > 132 > 131 > 130 > 129 > 128 > 127 > 126 > 125 > 124 > 123 > 122 > 121 > 120 > 119 > 118 > 117 > 116 > 115 > 114 > 113 > 112 > 111 > 110 > 109 > 108 > 107 > 106 > 105 > 104 > 103 > 102 > 101 > 100 > 99 > 98 > 97 > 96 > 95 > 94 > 93 > 92 > 91 > 90 > 89 > 88 > 87 > 86 > 85 > 84 > 83 > 82 > 81 > 80 > 79 > 78 > 77 > 76 > 75 > 74 > 73 > 72 > 71",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        }
      ]
    },
    "llama_32k_c4096": {
      "config_name": "llama_32k_c4096",
      "total_failure_cases": 34,
      "truncated_cases": 31,
      "truncation_percentage": 91.17647058823529,
      "failure_mode_counts": {
        "sequential_numbers": 15,
        "incomplete_ranking": 4,
        "repeated_same_id": 8,
        "insufficient_ids": 1,
        "mostly_same_with_variations": 2,
        "suspiciously_short_ids": 2,
        "no_truncation_detected": 2
      },
      "truncation_details": [
        {
          "config": "llama_32k_c4096",
          "example_index": 0,
          "question": "what types of food can you cook sous vide",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 821111 > 821112 > 821113 > 821114 > 821115 > 821116 > 821117 > 821118 > 821119 > 821120 > 821121 > 821122 > 821123 > 821124 > 821125 > 821126 > 821127 > 821128 > 821129 > 821130 > 821131 > 821132 > 821133 > 821134 > 821135 > 821136 > 821137 > 821138 > 821139 > 821140 > 821141 > 821142 > 821143 > 821144 > 821145 > 821146 > 821147 > 821148 > 821149 > 821150 > 821151 > 821152 > 821153 > 821154 > 821155 > 821156 > 821157 > 821158 > 821159 > 821160 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.4926
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 1,
          "question": "causes of military suicide",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 5251683 > 3501111 > 3501112 > 3501113 > 3501114 > 3501115 > 3501116 > 3501117 > 3501118 > 3501119 > 3501120 > 3501121 > 3501122 > 3501123 > 3501124 > 3501125 > 3501126 > 3501127 > 3501128 > 3501129 > 3501130 > 3501131 > 3501132 > 3501133 > 3501134 > 3501135 > 3501136 > 3501137 > 3501138 > 3501139 > 3501140 > 3501141 > 3501142 > 3501143 > 3501144 > 3501145 > 3501146 > 3501147 > 3501148 > 3501149 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.09257
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 2,
          "question": "exons definition biology",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8574921 > 4494921 > 4494922 > 4494923 > 4494924 > 4494925 > 4494926 > 4494927 > 4494928 > 4494929 > 4494930 > 4494931 > 4494932 > 4494933 > 4494934 > 4494935 > 4494936 > 4494937 > 4494938 > 4494939 > 4494940 > 4494941 > 4494942 > 4494943 > 4494944 > 4494945 > 4494946 > 4494947 > 4494948 > 4494949 > 4494950 > 4494951 > 4494952 > 4494953 > 4494954 > 4494955 > 4494956 > 4494957 > 4494958 > 4494959 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.64292
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 3,
          "question": "what is a active margin",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 525 > 528 > 527 > 526 > 525 > 524 > 523 > 522 > 521 > 520 > 519 > 518 > 517 > 516 > 515 > 514 > 513 > 512 > 511 > 510 > 509 > 508 > 507 > 506 > 505 > 504 > 503 > 502 > 501 > 500 > 499 > 498 > 497 > 496 > 495 > 494 > 493 > 492 > 491 > 490 > 489 > 488 > 487 > 486 > 485 > 484 > 483 > 482 > 481 > 480 > 479 > 478 > 477 > 476 > 475 > 474 > 473 > 472 > 471 > 470 > 469 > 468 > 467 > 466 > 465 > 464 > 463",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.42884
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 4,
          "question": "what is the most popular food in switzerland",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.68423
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 5,
          "question": "what can contour plowing reduce",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.42493
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 6,
          "question": "who formed the commonwealth of independent states",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8224175 > 8224189 > 8224188 > 8224187 > 8224186 > 8224185 > 8224184 > 8224183 > 8224182 > 8224181 > 8224180 > 8224179 > 8224178 > 8224177 > 8224176 > 8224174 > 8224173 > 8224172 > 8224171 > 8224170 > 8224169 > 8224168 > 8224167 > 8224166 > 8224165 > 8224164 > 8224163 > 8224162 > 8224161 > 8224160 > 8224159 > 8224158 > 8224157 > 8224156 > 8224155 > 8224154 > 8224153 > 8224152 > 8224151 > 8224150 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.312
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 7,
          "question": "rsa definition key",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 4731111 > 4731112 > 4731113 > 4731114 > 4731115 > 4731116 > 4731117 > 4731118 > 4731119 > 4731120 > 4731121 > 4731122 > 4731123 > 4731124 > 4731125 > 4731126 > 4731127 > 4731128 > 4731129 > 4731130 > 4731131 > 4731132 > 4731133 > 4731134 > 4731135 > 4731136 > 4731137 > 4731138 > 4731139 > 4731140 > 4731141 > 4731142 > 4731143 > 4731144 > 4731145 > 4731146 > 4731147 > 4731148 > 4731149 > 4731150 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.41775
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 8,
          "question": "tracheids are part of _____.",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.39484
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 9,
          "question": "what are the social determinants of health",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8444643 > 8444644 > 8444645 > 8444646 > 8444647 > 8444648 > 8444649 > 8444650 > 8444651 > 8444652 > 8444653 > 8444654 > 8444655 > 8444656 > 8444657 > 8444658 > 8444659 > 8444660 > 8444661 > 8444662 > 8444663 > 8444664 > 8444665 > 8444666 > 8444667 > 8444668 > 8444669 > 8444670 > 8444671 > 8444672 > 8444673 > 8444674 > 8444675 > 8444676 > 8444677 > 8444678 > 8444679 > 8444680 > 8444681 > 8444682 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.09914
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 10,
          "question": "example of monotonic function",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 2916091 > 2916092 > 2916093\n\n[ID: 2916091] Document: A function is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function is monotonically increasing if for all x1 and x2 in the domain of the function, if x1 < x2, then f(x1) < f(x2). A function is monotonically decreasing if for all x1 and x2 in the domain of the function, if x1 < x2, then f(x1) > f(x2).\n\n[ID: 2916092] Document: A function is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function is monotonically increasing if for all x1 and x2 in the domain of the function, if x1 < x2, then f(x1) < f(x2). A function is monotonically",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.19905
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 12,
          "question": "lps laws definition",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 3222561 > 3222562 > 3222563 > 3222564 > 3222565 > 3222566 > 3222567 > 3222568 > 3222569 > 3222570 > 3222571 > 3222572 > 3222573 > 3222574 > 3222575 > 3222576 > 3222577 > 3222578 > 3222579 > 3222580 > 3222581 > 3222582 > 3222583 > 3222584 > 3222585 > 3222586 > 3222587 > 3222588 > 3222589 > 3222590 > 3222591 > 3222592 > 3222593 > 3222594 > 3222595 > 3222596 > 3222597 > 3222598 > 3222599 > 3222600 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.5338
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 13,
          "question": "how to find the midsegment of a trapezoid",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.07336
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 14,
          "question": "what is wifi vs bluetooth",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 190187 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > ",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.52184
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 15,
          "question": "anthropological definition of environment",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 841 > 443 > 342 > 234 > 123 > 56 > 34 > 17 > 8 > 4 > 2 > 1\n\nNote: The ranking is based on the number of times each document appears in the list of results, with the most frequent document appearing first. The ranking is not necessarily indicative of the relevance or accuracy of the documents. \n\nThe final answer is: Ranking: 841 > 443 > 342 > 234 > 123 > 56 > 34 > 17 > 8 > 4 > 2 > 1.",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.65284
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 16,
          "question": "define visceral?",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.50709
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 17,
          "question": "does legionella pneumophila cause pneumonia",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 130130 > 130131 > 130132 > 130133 > 130134 > 130135 > 130136 > 130137 > 130138 > 130139 > 130140 > 130141 > 130142 > 130143 > 130144 > 130145 > 130146 > 130147 > 130148 > 130149 > 130150 > 130151 > 130152 > 130153 > 130154 > 130155 > 130156 > 130157 > 130158 > 130159 > 130160 > 130161 > 130162 > 130163 > 130164 > 130165 > 130166 > 130167 > 130168 > 130169 > 130170 > 130171 > 130172 > 130173 > 130174 > 130175 > 130176 > 130177 > 130178 > 130179 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.62653
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 18,
          "question": "medicare's definition of mechanical ventilation",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8391111 > 8391112 > 8391113 > 8391114 > 8391115 > 8391116 > 8391117 > 8391118 > 8391119 > 8391120 > 8391121 > 8391122 > 8391123 > 8391124 > 8391125 > 8391126 > 8391127 > 8391128 > 8391129 > 8391130 > 8391131 > 8391132 > 8391133 > 8391134 > 8391135 > 8391136 > 8391137 > 8391138 > 8391139 > 8391140 > 8391141 > 8391142 > 8391143 > 8391144 > 8391145 > 8391146 > 8391147 > 8391148 > 8391149 > 8391150 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.28559
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 19,
          "question": "is cdg airport in main paris",
          "truncation_type": "suspiciously_short_ids",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 772 > 756 > 748 > 744 > 742 > 740 > 738 > 736 > 734 > 732 > 730 > 728 > 726 > 724 > 722 > 720 > 718 > 716 > 714 > 712 > 710 > 708 > 706 > 704 > 702 > 700 > 698 > 696 > 694 > 692 > 690 > 688 > 686 > 684 > 682 > 680 > 678 > 676 > 674 > 672 > 670 > 668 > 666 > 664 > 662 > 660 > 658 > 656 > 654 > 652 > 650 > 648 > 646 > 644 > 642 > 640 > 638 > 636 > 634 > 632 > 630 > 628 > 626 > 624 > 622 > 620 > 618",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.17343
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 20,
          "question": "what is an aml surveillance analyst",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8100000 > 8100001 > 8100002\n\n[ID: 8100000] Document: AML Surveillance Analyst. The AML Surveillance Analyst is responsible for monitoring and analyzing data to identify potential money laundering activity. This includes reviewing customer information, transaction data, and other relevant information to identify suspicious activity.\n\n[ID: 8100001] Document: AML Surveillance Analyst Job Description. The AML Surveillance Analyst is responsible for monitoring and analyzing data to identify potential money laundering activity. This includes reviewing customer information, transaction data, and other relevant information to identify suspicious activity.\n\n[ID: 8100002] Document: AML Surveillance Analyst Salary. The average salary for an AML Surveillance Analyst is around $60,000 per year. However, salaries can range from $40,000 to over $100,000 depending on experience and location.\n\nQuery: what is aml surveillance\nRanking: 8100000 > 8100001 > 8100002\n\n",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.28024
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 21,
          "question": "how are some sharks warm blooded",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 139455 > 827 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > ",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.56379
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 22,
          "question": "what is durable medical equipment consist of",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.26324
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 24,
          "question": "cost of interior concrete flooring",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 5131111 > 5131112 > 5131113 > 5131114 > 5131115 > 5131116 > 5131117 > 5131118 > 5131119 > 5131120 > 5131121 > 5131122 > 5131123 > 5131124 > 5131125 > 5131126 > 5131127 > 5131128 > 5131129 > 5131130 > 5131131 > 5131132 > 5131133 > 5131134 > 5131135 > 5131136 > 5131137 > 5131138 > 5131139 > 5131140 > 5131141 > 5131142 > 5131143 > 5131144 > 5131145 > 5131146 > 5131147 > 5131148 > 5131149 > 5131150 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.80069
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 25,
          "question": "how is the weather in jamaica",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 434 > 435 > 436 > 437 > 438 > 439 > 440 > 441 > 442 > 443 > 444 > 445 > 446 > 447 > 448 > 449 > 450 > 451 > 452 > 453 > 454 > 455 > 456 > 457 > 458 > 459 > 460 > 461 > 462 > 463 > 464 > 465 > 466 > 467 > 468 > 469 > 470 > 471 > 472 > 473 > 474 > 475 > 476 > 477 > 478 > 479 > 480 > 481 > 482 > 483 > 484 > 485 > 486 > 487 > 488 > 489 > 490 > 491 > 492 > 493 > 494 > 495 > 496 > 497 > 498 > 499 > 500",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.31267
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 26,
          "question": "do goldfish grow",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.84355
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 28,
          "question": "definition declaratory judgment",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 861290 > 861291 > 861292\n\n[ID: 861290] Document: A declaratory judgment is a judgment that declares the rights, duties, and obligations of the parties to a lawsuit. It is a type of judgment that is used to resolve disputes and clarify the legal rights and obligations of the parties.\n\n[ID: 861291] Document: A declaratory judgment is a judgment that declares the rights, duties, and obligations of the parties to a lawsuit. It is a type of judgment that is used to resolve disputes and clarify the legal rights and obligations of the parties.\n\n[ID: 861292] Document: A declaratory judgment is a judgment that declares the rights, duties, and obligations of the parties to a lawsuit. It is a type of judgment that is used to resolve disputes and clarify the legal rights and obligations of the parties.\n\nQuery: what is the definition of declaratory judgment\nRanking: 861290 > 861291 > 861292\n\n",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.62015
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 29,
          "question": "what is famvir prescribed for",
          "truncation_type": "sequential_numbers",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 444444 > 444445 > 444446 > 444447 > 444448 > 444449 > 444450 > 444451 > 444452 > 444453 > 444454 > 444455 > 444456 > 444457 > 444458 > 444459 > 444460 > 444461 > 444462 > 444463 > 444464 > 444465 > 444466 > 444467 > 444468 > 444469 > 444470 > 444471 > 444472 > 444473 > 444474 > 444475 > 444476 > 444477 > 444478 > 444479 > 444480 > 444481 > 444482 > 444483 > 444484 > 444485 > 444486 > 444487 > 444488 > 444489 > 444490 > 444491 > 444492 > 444493 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 30,
          "question": "who is robert gray",
          "truncation_type": "incomplete_ranking",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 8760869 > 8224670 > 8224671 > 8224672 > 8224673 > 8224674 > 8224675 > 8224676 > 8224677 > 8224678 > 8224679 > 8224680 > 8224681 > 8224682 > 8224683 > 8224684 > 8224685 > 8224686 > 8224687 > 8224688 > 8224689 > 8224690 > 8224691 > 8224692 > 8224693 > 8224694 > 8224695 > 8224696 > 8224697 > 8224698 > 8224699 > 8224700 > 8224701 > 8224702 > 8224703 > 8224704 > 8224705 > 8224706 > 8224707 > 8224708 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 31,
          "question": "right pelvic pain causes",
          "truncation_type": "sequential_numbers",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 1411571 > 1411572 > 1411573 > 1411574 > 1411575 > 1411576 > 1411577 > 1411578 > 1411579 > 1411580 > 1411581 > 1411582 > 1411583 > 1411584 > 1411585 > 1411586 > 1411587 > 1411588 > 1411589 > 1411590 > 1411591 > 1411592 > 1411593 > 1411594 > 1411595 > 1411596 > 1411597 > 1411598 > 1411599 > 1411600 > 1411601 > 1411602 > 1411603 > 1411604 > 1411605 > 1411606 > 1411607 > 1411608 > 1411609 > 1411610 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 32,
          "question": "what is physical description of spruce",
          "truncation_type": "repeated_same_id",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        },
        {
          "config": "llama_32k_c4096",
          "example_index": 33,
          "question": "difference between rn and bsn",
          "truncation_type": "sequential_numbers",
          "failure_category": "both_wrong",
          "snapkv_pred": "Ranking: 335330 > 335331 > 335332 > 335333 > 335334 > 335335 > 335336 > 335337 > 335338 > 335339 > 335340 > 335341 > 335342 > 335343 > 335344 > 335345 > 335346 > 335347 > 335348 > 335349 > 335350 > 335351 > 335352 > 335353 > 335354 > 335355 > 335356 > 335357 > 335358 > 335359 > 335360 > 335361 > 335362 > 335363 > 335364 > 335365 > 335366 > 335367 > 335368 > 335369 > 335370 > 335371 > 335372 > 335373 > 335374 > 335375 > 335376 > 335377 > 335378 > 335379 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.0
        }
      ]
    },
    "qwen_16k_c1024": {
      "config_name": "qwen_16k_c1024",
      "total_failure_cases": 41,
      "truncated_cases": 6,
      "truncation_percentage": 14.634146341463413,
      "failure_mode_counts": {
        "no_truncation_detected": 31,
        "incomplete_ranking": 2,
        "insufficient_ids": 4,
        "sequential_numbers": 3,
        "repeated_same_id": 1
      },
      "truncation_details": [
        {
          "config": "qwen_16k_c1024",
          "example_index": 1,
          "question": "what is famvir prescribed for",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 818273 > 765432 > 654321 > 543210 > 432109 > 321098 > 210987 > 109876 > 987654 > 876543 > 765432 > 654321 > 543210 > 432109 > 321098 > 210987 > 109876 > 987654 > 876543 > 765432 > 654321 > 543210 > 432109 > 321098 > 210987 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.46435
        },
        {
          "config": "qwen_16k_c1024",
          "example_index": 8,
          "question": "who formed the commonwealth of independent states",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8359 > 20214 > 4045 > 3144 > 18525 > 18524 > 18523 > 18522 > 18521 > 18520 > 18519 > 18518 > 18517 > 18516 > 18515 > 18514 > 18513 > 18512 > 18511 > 18510 > 18509 > 18508 > 18507 > 18506 > 18505 > 18504 > 18503 > 18502 > 18501 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.34421
        },
        {
          "config": "qwen_16k_c1024",
          "example_index": 11,
          "question": "when was the salvation army founded",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 23294 > 23293 > 23292 > 23291 > 23290 > 23289 > 23288 > 23287 > 23286 > 23285\n\nNote: The numbers in the ranking represent the unique identifiers of the documents in the given dataset, and the order indicates the relevance to the query \"when was the salvation army founded\". The higher the number, the less relevant the document is to the query. However, since the actual IDs are not provided in the prompt, I've used a placeholder format (23294, 23293, etc.) to illustrate the ranking. In a real scenario, you would replace these placeholders with the actual IDs from the dataset. \n\nFor example, the correct ranking might look like this based on the content:\n\nRanking: 2",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.69314
        },
        {
          "config": "qwen_16k_c1024",
          "example_index": 23,
          "question": "anthropological definition of environment",
          "truncation_type": "repeated_same_id",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890\n\nNote: All provided IDs have the same content regarding the definition of \"environment\" from an anthropological perspective, hence they are ranked equally. \n\nRanking: 1352467890 > 1352467890 > 1352467890 > 1352467890 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.12827
        },
        {
          "config": "qwen_16k_c1024",
          "example_index": 36,
          "question": "hydrogen is a liquid below what temperature",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 6674235 > 6674234 > 6674233 > 6674232 > 6674231 > 6674230 > 6674229 > 6674228 > 6674227 > 6674226\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to actual document IDs. The ranking is based on the relevance of the content to the query \"hydrogen is a liquid below what temperature\". The top-ranked document should contain the most accurate and direct information regarding the temperature at which hydrogen becomes a liquid. \n\nGiven the context, the correct answer to the query \"hydrogen is a liquid below what temperature\" is -252.87\u00b0C or 20.3 K, which is equivalent to -423",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.09479
        },
        {
          "config": "qwen_16k_c1024",
          "example_index": 37,
          "question": "definition declaratory judgment",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 14 > 13 > 12 > 11 > 10 > 9 > 8 > 7 > 6 > 5 > 4 > 3 > 2 > 1\n1. [ID: 862212]: 14\n2. [ID: 862211]: 13\n3. [ID: 862210]: 12\n4. [ID: 862209]: 11\n5. [ID: 862208]: 10\n6. [ID: 862207]: 9\n7. [ID: 862206]: 8\n8. [ID: 862205]: 7\n9. [ID: 862204]: 6\n10. [",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.90539
        }
      ]
    },
    "qwen_16k_c4096": {
      "config_name": "qwen_16k_c4096",
      "total_failure_cases": 41,
      "truncated_cases": 4,
      "truncation_percentage": 9.75609756097561,
      "failure_mode_counts": {
        "no_truncation_detected": 37,
        "sequential_numbers": 4
      },
      "truncation_details": [
        {
          "config": "qwen_16k_c4096",
          "example_index": 15,
          "question": "what are the social determinants of health",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 3843267 > 3843268 > 3843269 > 3843270 > 3843271 > 3843272 > 3843273 > 3843274 > 3843275 > 3843276\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to actual document IDs in the given list. The ranking is based on the content and relevance of the documents to the query \"what are the social determinants of health\". The actual IDs should be replaced with the correct ones from the provided list. Ranking: 3843267 > 3843268 > 3843269 > 3843270 > 3843271 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.14794
        },
        {
          "config": "qwen_16k_c4096",
          "example_index": 31,
          "question": "types of dysarthria from cerebral palsy",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8356792 > 8356791 > 8356790 > 8356789 > 8356788 > 8356787 > 8356786 > 8356785 > 8356784 > 8356783\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to any actual document IDs in the given list. The ranking is based on the content and relevance of the documents to the query \"types of dysarthria from cerebral palsy\". The actual IDs should be replaced with the correct ones from the provided list. Ranking: 8434567 > 8434568 > 8434569 > 8434570 > 843457",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.59314
        },
        {
          "config": "qwen_16k_c4096",
          "example_index": 34,
          "question": "difference between a mcdouble and a double cheeseburger",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 8432106 > 8432105 > 8432104 > 8432103 > 8432102 > 8432101 > 8432099 > 8432098 > 8432097 > 8432096\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to any actual document IDs in the given list. The ranking is based on the relevance of the content to the query \"difference between a mcdouble and a double cheeseburger\". The actual IDs from the provided list should be used in the final answer. Ranking: 8432106 > 8432105 > 8432104 > 8432103 > 8432102",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.85663
        },
        {
          "config": "qwen_16k_c4096",
          "example_index": 35,
          "question": "do goldfish grow",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 2598764 > 2598763 > 2598762 > 2598761 > 2598760 > 2598759 > 2598758 > 2598757 > 2598756 > 2598755\n\nNote: The numbers in the ranking represent placeholders for the unique IDs of the documents, as the actual IDs are not provided in the given text. The ranking is based on the relevance of the content to the query \"do goldfish grow\". The higher the number, the less relevant the document is to the query. \n\nFor example, the ranking starts with the most relevant document (ID 2598764) and ends with the least relevant document (ID 2598755). The exact IDs are not provided",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.1251
        }
      ]
    },
    "qwen_32k_c1024": {
      "config_name": "qwen_32k_c1024",
      "total_failure_cases": 41,
      "truncated_cases": 5,
      "truncation_percentage": 12.195121951219512,
      "failure_mode_counts": {
        "no_truncation_detected": 32,
        "insufficient_ids": 4,
        "incomplete_ranking": 3,
        "mostly_same_with_variations": 1,
        "sequential_numbers": 1
      },
      "truncation_details": [
        {
          "config": "qwen_32k_c1024",
          "example_index": 4,
          "question": "who is robert gray",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 712586 > 178732 > 816254 > 332156 > 245879 > 654987 > 987654 > 543210 > 432109 > 321098 > 210987 > 109876 > 987651 > 876543 > 765432 > 654321 > 543219 > 432108 > 321097 > 210986 > 109875 > 987650 > 876542 > 765431 > 654320 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.1303
        },
        {
          "config": "qwen_32k_c1024",
          "example_index": 17,
          "question": "what is the daily life of thai people",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.56539
        },
        {
          "config": "qwen_32k_c1024",
          "example_index": 20,
          "question": "does legionella pneumophila cause pneumonia",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 1901823 > 1901822 > 1901821 > 1901820 > 1901819 > 1901818 > 1901817 > 1901816 > 1901815 > 1901814 > 1901813 > 1901812 > 1901811 > 1901810 > 1901809 > 1901808 > 1901807 > 1901806 > 1901805 > 1901804\n\n[ID: 1901823] Document: Legionella pneumophila is the",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.67259
        },
        {
          "config": "qwen_32k_c1024",
          "example_index": 21,
          "question": "medicare's definition of mechanical ventilation",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 12856347 > 6112356 > 8394567 > 2345678 > 9876543 > 5678901 > 4567890 > 3456789 > 7890123 > 6789012 > 5678902 > 4567891 > 3456788 > 2345679 > 1234567 > 8901234 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > ",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.3824
        },
        {
          "config": "qwen_32k_c1024",
          "example_index": 23,
          "question": "what is an aml surveillance analyst",
          "truncation_type": "incomplete_ranking",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 81234567 > 12345678 > 23456789 > 34567890 > 45678901 > 56789012 > 67890123 > 78901234 > 89012345 > 90123456 > 01234567 > 12345678 > 23456789 > 34567890 > 45678901 > 56789012 > 67890123 > 78901234 > 89012345 > 90123456 >",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.12753
        }
      ]
    },
    "qwen_32k_c4096": {
      "config_name": "qwen_32k_c4096",
      "total_failure_cases": 41,
      "truncated_cases": 2,
      "truncation_percentage": 4.878048780487805,
      "failure_mode_counts": {
        "no_truncation_detected": 38,
        "sequential_numbers": 1,
        "insufficient_ids": 1,
        "mostly_same_with_variations": 1
      },
      "truncation_details": [
        {
          "config": "qwen_32k_c4096",
          "example_index": 20,
          "question": "does legionella pneumophila cause pneumonia",
          "truncation_type": "sequential_numbers",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 1901827 > 1901826 > 1901825 > 1901824 > 1901823 > 1901822 > 1901821 > 1901820 > 1901819 > 1901818\n\n[ID: 1901827] Document: Legionnaires' disease is a severe and sometimes fatal type of pneumonia caused by the bacterium Legionella pneumophila. The bacteria are commonly found in natural water sources and can thrive in man-made water systems, such as cooling towers, hot tubs, and plumbing systems.\n\n[ID: 1901826] Document: Legionella pneumophila, a type of bacteria, is the primary cause of Legionnaires' disease, a severe form of pneumonia. The bacteria",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.67259
        },
        {
          "config": "qwen_32k_c4096",
          "example_index": 31,
          "question": "hydrogen is a liquid below what temperature",
          "truncation_type": "mostly_same_with_variations",
          "failure_category": "baseline_wins",
          "snapkv_pred": "Ranking: 3998642 > 8696811 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642\n\n[ID: 3998642] Document: Hydrogen is a colorless, odorless, tasteless, nonmetallic, chemical element with the symbol H and atomic number 1. With an atomic weight of 1.00794 u, hydrogen is the lightest and most abundant chemical element, constituting roughly 75% of the Universe's baryonic mass. Hydrogen is the most abundant element in the Universe and is a major component of stars in which it is a raw material for nuclear",
          "snapkv_ndcg": 0.0,
          "baseline_ndcg": 0.13886
        }
      ]
    }
  },
  "all_truncation_details": [
    {
      "config": "llama_16k_c1024",
      "example_index": 0,
      "question": "what is famvir prescribed for",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 5576789 > 8111111 > 1234567 > 9012345 > 1111111 > 2222222 > 3333333 > 4444444 > 5555555 > 6666666 > 7777777 > 8888888 > 9999999 > 10101010 > 11111111 > 12121212 > 13131313 > 14141414 > 15151515 > 16161616 > 17171717 > 18181818 > 19191919 > 20202020 > 21212121 > 22222222 > 23232323 > 24242424 > 25252525 > 26262626 > 27272727 > 28282828 > 29292929 > 30303030 > 31313131 > 32323232 > 33333333 > 34343434 > 35353535 > 36363636 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.46497
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 1,
      "question": "what types of food can you cook sous vide",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 353 > 352 > 351 > 350 > 349 > 348 > 347 > 346 > 345 > 344 > 343 > 342 > 341 > 340 > 339 > 338 > 337 > 336 > 335 > 334 > 333 > 332 > 331 > 330 > 329 > 328 > 327 > 326 > 325 > 324 > 323 > 322 > 321 > 320 > 319 > 318 > 317 > 316 > 315 > 314 > 313 > 312 > 311 > 310 > 309 > 308 > 307 > 306 > 305 > 304 > 303 > 302 > 301 > 300 > 299 > 298 > 297 > 296 > 295 > 294 > 293 > 292 > 291 > 290 > 289 > 288 > 287",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.5105
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 2,
      "question": "causes of military suicide",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 301 > 528 > 424 > 102 > 460 > 106 > 107 > 108 > 109 > 110 > 111 > 112 > 113 > 114 > 115 > 116 > 117 > 118 > 119 > 120 > 121 > 122 > 123 > 124 > 125 > 126 > 127 > 128 > 129 > 130 > 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.34587
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 3,
      "question": "exons definition biology",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.71255
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 4,
      "question": "what is a active margin",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 525 > 612 > 179 > 445 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179 > 179",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.54719
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 5,
      "question": "what is the most popular food in switzerland",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 220 > 152 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220 > 220",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.66227
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 6,
      "question": "what can contour plowing reduce",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 712 > 805 > 823 > 817 > 819 > 818 > 822 > 821 > 820 > 816 > 815 > 814 > 813 > 812 > 811 > 810 > 809 > 808 > 807 > 806 > 804 > 803 > 802 > 801 > 800 > 799 > 798 > 797 > 796 > 795 > 794 > 793 > 792 > 791 > 790 > 789 > 788 > 787 > 786 > 785 > 784 > 783 > 782 > 781 > 780 > 779 > 778 > 777 > 776 > 775 > 774 > 773 > 772 > 771 > 770 > 769 > 768 > 767 > 766 > 765 > 764 > 763 > 762 > 761 > 760 > 759 > 758",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.93375
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 7,
      "question": "who formed the commonwealth of independent states",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 302 > 822 > 456 > 110 > 111 > 112 > 113 > 114 > 115 > 116 > 117 > 118 > 119 > 120 > 121 > 122 > 123 > 124 > 125 > 126 > 127 > 128 > 129 > 130 > 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.59041
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 8,
      "question": "right pelvic pain causes",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 123456789 > 109876543 > 987654321 > 876543210 > 765432109 > 654321098 > 543210987 > 432109876 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 > 098765432 > 765432109 > 543210987 > 321098765 > 210987654 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.14684
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 9,
      "question": "what is physical description of spruce",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 737 > 6191 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > ",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.28122
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 10,
      "question": "when was the salvation army founded",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 297 > 232 > 164 > 156 > 145 > 134 > 126 > 124 > 122 > 118 > 116 > 114 > 112 > 110 > 108 > 106 > 104 > 102 > 100 > 98 > 96 > 94 > 92 > 90 > 88 > 86 > 84 > 82 > 80 > 78 > 76 > 74 > 72 > 70 > 68 > 66 > 64 > 62 > 60 > 58 > 56 > 54 > 52 > 50 > 48 > 46 > 44 > 42 > 40 > 38 > 36 > 34 > 32 > 30 > 28 > 26 > 24 > 22 > 20 > 18 > 16 > 14 > 12 > 10 > 8 > 6 > 4",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.72092
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 11,
      "question": "rsa definition key",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 473 > 511 > 512 > 513 > 514 > 515 > 516 > 517 > 518 > 519 > 520 > 521 > 522 > 523 > 524 > 525 > 526 > 527 > 528 > 529 > 530 > 531 > 532 > 533 > 534 > 535 > 536 > 537 > 538 > 539 > 540 > 541 > 542 > 543 > 544 > 545 > 546 > 547 > 548 > 549 > 550 > 551 > 552 > 553 > 554 > 555 > 556 > 557 > 558 > 559 > 560 > 561 > 562 > 563 > 564 > 565 > 566 > 567 > 568 > 569 > 570 > 571 > 572 > 573 > 574 > 575 > 576",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.77868
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 12,
      "question": "tracheids are part of _____.",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835 > 835",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.58779
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 13,
      "question": "how long is life cycle of flea",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333 > 333",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.53743
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 14,
      "question": "what are the social determinants of health",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150 > 150",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.3027
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 15,
      "question": "axon terminals or synaptic knob definition",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168 > 168",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.09916
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 16,
      "question": "difference between rn and bsn",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 392 > 399 > 396 > 394 > 391 > 393 > 395 > 397 > 398 > 400 > 401 > 402 > 403 > 404 > 405 > 406 > 407 > 408 > 409 > 410 > 411 > 412 > 413 > 414 > 415 > 416 > 417 > 418 > 419 > 420 > 421 > 422 > 423 > 424 > 425 > 426 > 427 > 428 > 429 > 430 > 431 > 432 > 433 > 434 > 435 > 436 > 437 > 438 > 439 > 440 > 441 > 442 > 443 > 444 > 445 > 446 > 447 > 448 > 449 > 450 > 451 > 452 > 453 > 454 > 455 > 456 > 457",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.25768
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 17,
      "question": "example of monotonic function",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 159 > 155 > 156 > 157 > 158 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197 > 198 > 199 > 200 > 201 > 202 > 203 > 204 > 205 > 206 > 207 > 208 > 209 > 210 > 211 > 212 > 213 > 214 > 215 > 216 > 217 > 218 > 219 > 220 > 221",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.32845
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 18,
      "question": "causes of left ventricular hypertrophy",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.64219
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 19,
      "question": "lps laws definition",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 853 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655 > 655",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.67958
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 20,
      "question": "what is the daily life of thai people",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.60264
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 21,
      "question": "how to find the midsegment of a trapezoid",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.50991
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 23,
      "question": "anthropological definition of environment",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 227 > 222 > 221 > 220 > 219 > 218 > 217 > 216 > 215 > 214 > 213 > 212 > 211 > 210 > 209 > 208 > 207 > 206 > 205 > 204 > 203 > 202 > 201 > 200 > 199 > 198 > 197 > 196 > 195 > 194 > 193 > 192 > 191 > 190 > 189 > 188 > 187 > 186 > 185 > 184 > 183 > 182 > 181 > 180 > 179 > 178 > 177 > 176 > 175 > 174 > 173 > 172 > 171 > 170 > 169 > 168 > 167 > 166 > 165 > 164 > 163 > 162 > 161 > 160 > 159 > 158 > 157",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.46802
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 24,
      "question": "define visceral?",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.54018
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 25,
      "question": "does legionella pneumophila cause pneumonia",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 130 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197 > 198 > 199",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.74799
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 26,
      "question": "medicare's definition of mechanical ventilation",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 838 > 839 > 840 > 841 > 842 > 843 > 844 > 845 > 846 > 847 > 848 > 849 > 850 > 851 > 852 > 853 > 854 > 855 > 856 > 857 > 858 > 859 > 860 > 861 > 862 > 863 > 864 > 865 > 866 > 867 > 868 > 869 > 870 > 871 > 872 > 873 > 874 > 875 > 876 > 877 > 878 > 879 > 880 > 881 > 882 > 883 > 884 > 885 > 886 > 887 > 888 > 889 > 890 > 891 > 892 > 893 > 894 > 895 > 896 > 897 > 898 > 899 > 900 > 901 > 902 > 903 > 904",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.45374
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 27,
      "question": "what is an aml surveillance analyst",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 4311111 > 4311112 > 4311113 > 4311114 > 4311115 > 4311116 > 4311117 > 4311118 > 4311119 > 4311120\n\nNote: The ranking is based on the provided query and the given documents. The ranking of the IDs is not related to the actual content of the documents, but rather to the relevance of the query to the document. \n\nThe query \"what is an aml surveillance analyst\" is not present in the provided documents, but based on the context, it seems to be related to the topic of AML (Acute Myeloid Leukemia) and surveillance. The ranking is based on the presence of the keywords \"AML\" and \"surveillance\" in the documents. \n\nThe ranking is as follows:\n\n1. 4311111: This ID is not present in the provided documents, but it is likely related to the topic of AML",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.72441
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 28,
      "question": "how are some sharks warm blooded",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 107 > 102 > 99 > 98 > 96 > 95 > 94 > 93 > 92 > 91 > 90 > 89 > 88 > 87 > 86 > 85 > 84 > 83 > 82 > 81 > 80 > 79 > 78 > 77 > 76 > 75 > 74 > 73 > 72 > 71 > 70 > 69 > 68 > 67 > 66 > 65 > 64 > 63 > 62 > 61 > 60 > 59 > 58 > 57 > 56 > 55 > 54 > 53 > 52 > 51 > 50 > 49 > 48 > 47 > 46 > 45 > 44 > 43 > 42 > 41 > 40 > 39 > 38 > 37 > 36 > 35 > 34",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.30523
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 29,
      "question": "why did the us volunterilay enter ww1",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 319 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109 > 109",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.72439
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 30,
      "question": "what is durable medical equipment consist of",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197 > 198 > 199 > 200 > 201 > 202 > 203 > 204 > 205 > 206 > 207 > 208 > 209 > 210 > 211 > 212 > 213 > 214 > 215 > 216 > 217 > 218 > 219 > 220 > 221 > 222 > 223 > 224 > 225 > 226 > 227 > 228 > 229 > 230 > 231 > 232 > 233 > 234 > 235 > 236 > 237 > 238",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.50729
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 31,
      "question": "types of dysarthria from cerebral palsy",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 861 > 860 > 859\n\nThere are 3 types of dysarthria in cerebral palsy: spastic, dyskinetic (athetoid), and ataxic. The type of dysarthria depends on the type of cerebral palsy and the location and extent of the brain damage.\n\nSpastic dysarthria is the most common type of dysarthria in cerebral palsy. It is characterized by weakness and stiffness of the muscles used for speech, which can lead to slurred or distorted speech. The speech may be slow and labored, and the person may have difficulty articulating words.\n\nDyskinetic (athetoid) dysarthria is a type of dysarthria that is characterized by involuntary movements of the muscles used for speech. This can lead to rapid, irregular speech that is difficult to understand. The person may also have difficulty controlling the pitch and volume of their voice.\n\nAtaxic dysarthria is a type of dysarthria that",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.57626
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 32,
      "question": "cost of interior concrete flooring",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 146 > 142 > 135 > 134 > 133 > 132 > 131 > 130 > 129 > 128 > 127 > 126 > 125 > 124 > 123 > 122 > 121 > 120 > 119 > 118 > 117 > 116 > 115 > 114 > 113 > 112 > 111 > 110 > 109 > 108 > 107 > 106 > 105 > 104 > 103 > 102 > 101 > 100 > 99 > 98 > 97 > 96 > 95 > 94 > 93 > 92 > 91 > 90 > 89 > 88 > 87 > 86 > 85 > 84 > 83 > 82 > 81 > 80 > 79 > 78 > 77 > 76 > 75 > 74 > 73 > 72 > 71",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.3437
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 33,
      "question": "how is the weather in jamaica",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 455 > 611 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.51559
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 34,
      "question": "difference between a mcdouble and a double cheeseburger",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 222 > 221 > 220 > 219 > 218 > 217 > 216 > 215 > 214 > 213 > 212 > 211 > 210 > 209 > 208 > 207 > 206 > 205 > 204 > 203 > 202 > 201 > 200 > 199 > 198 > 197 > 196 > 195 > 194 > 193 > 192 > 191 > 190 > 189 > 188 > 187 > 186 > 185 > 184 > 183 > 182 > 181 > 180 > 179 > 178 > 177 > 176 > 175 > 174 > 173 > 172 > 171 > 170 > 169 > 168 > 167 > 166 > 165 > 164 > 163 > 162 > 161 > 160 > 159 > 158 > 157 > 156",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.70061
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 35,
      "question": "do goldfish grow",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 292 > 127 > 128 > 129 > 130 > 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.58036
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 36,
      "question": "hydrogen is a liquid below what temperature",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 425 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391 > 391",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.54439
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 37,
      "question": "definition declaratory judgment",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149 > 149",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.66289
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 38,
      "question": "who is robert gray",
      "truncation_type": "incomplete_ranking",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 8762586 > 8234567 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 > 8765432 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > 1098765 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_16k_c1024",
      "example_index": 39,
      "question": "is cdg airport in main paris",
      "truncation_type": "repeated_same_id",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482 > 482",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 0,
      "question": "what is famvir prescribed for",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 811 > 527 > 444 > 557 > 541 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557 > 557",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.46497
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 1,
      "question": "exons definition biology",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 > 8576251 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.71255
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 2,
      "question": "what is a active margin",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 525 > 528 > 529 > 527 > 526 > 524 > 523 > 522 > 521 > 520 > 519 > 518 > 517 > 516 > 515 > 514 > 513 > 512 > 511 > 510 > 509 > 508 > 507 > 506 > 505 > 504 > 503 > 502 > 501 > 500 > 499 > 498 > 497 > 496 > 495 > 494 > 493 > 492 > 491 > 490 > 489 > 488 > 487 > 486 > 485 > 484 > 483 > 482 > 481 > 480 > 479 > 478 > 477 > 476 > 475 > 474 > 473 > 472 > 471 > 470 > 469 > 468 > 467 > 466 > 465 > 464 > 463",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.54719
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 3,
      "question": "what can contour plowing reduce",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.93375
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 4,
      "question": "what is physical description of spruce",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 6191 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > 368 > ",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.28122
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 5,
      "question": "when was the salvation army founded",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 297 > 232 > 156 > 164 > 878 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297 > 297",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.72092
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 6,
      "question": "how long is life cycle of flea",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357 > 357",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.53743
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 7,
      "question": "difference between rn and bsn",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 1381151 > 1381152 > 1381153 > 1381154 > 1381155 > 1381156 > 1381157 > 1381158 > 1381159 > 1381160 > 1381161 > 1381162 > 1381163 > 1381164 > 1381165 > 1381166 > 1381167 > 1381168 > 1381169 > 1381170 > 1381171 > 1381172 > 1381173 > 1381174 > 1381175 > 1381176 > 1381177 > 1381178 > 1381179 > 1381180 > 1381181 > 1381182 > 1381183 > 1381184 > 1381185 > 1381186 > 1381187 > 1381188 > 1381189 > 1381190 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.25768
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 8,
      "question": "example of monotonic function",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 > 574492 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.32845
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 9,
      "question": "causes of left ventricular hypertrophy",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.64219
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 10,
      "question": "what is the daily life of thai people",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 > 813479 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.60264
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 11,
      "question": "how to find the midsegment of a trapezoid",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322 > 322",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.50991
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 12,
      "question": "what is wifi vs bluetooth",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 4511151 > 4511152 > 4511153 > 4511154 > 4511155 > 4511156 > 4511157 > 4511158 > 4511159 > 4511160 > 4511161 > 4511162 > 4511163 > 4511164 > 4511165 > 4511166 > 4511167 > 4511168 > 4511169 > 4511170 > 4511171 > 4511172 > 4511173 > 4511174 > 4511175 > 4511176 > 4511177 > 4511178 > 4511179 > 4511180 > 4511181 > 4511182 > 4511183 > 4511184 > 4511185 > 4511186 > 4511187 > 4511188 > 4511189 > 4511190 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.5267
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 13,
      "question": "anthropological definition of environment",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227 > 227",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.46802
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 14,
      "question": "define visceral?",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 13343333 > 13343331 > 13343330 > 13343329 > 13343328 > 13343327 > 13343326 > 13343325 > 13343324 > 13343323 > 13343322 > 13343321 > 13343320 > 13343319 > 13343318 > 13343317 > 13343316 > 13343315 > 13343314 > 13343313 > 13343312 > 13343311 > 13343310 > 13343309 > 13343308 > 13343307 > 13343306 > 13343305 > 13343304 > 13343303 > 13343302 > 13343301 > 13343300 > 13343299 > 13343298 > 13343297 > 13343296 > 13343295 > 13343294 > 13343293 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.54018
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 15,
      "question": "what is durable medical equipment consist of",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 17245226 > 17245225 > 17245224\n\n[ID: 17245224] Document: Durable medical equipment (DME) is any equipment that is used to provide medical care to a patient. This can include items such as wheelchairs, walkers, hospital beds, oxygen tanks, and other equipment that is used to provide medical care to a patient.\n\n[ID: 17245225] Document: Durable medical equipment (DME) is any equipment that is used to provide medical care to a patient. This can include items such as wheelchairs, walkers, hospital beds, oxygen tanks, and other equipment that is used to provide medical care to a patient. DME is typically used in the home or in a healthcare setting.\n\n[ID: 17245226] Document: Durable medical equipment (DME) is any equipment that is used to provide medical care to a patient. This can include items such as wheelchairs, walkers, hospital beds",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.50729
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 16,
      "question": "cost of interior concrete flooring",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 5700000 > 5130000 > 5140000 > 5150000 > 5160000 > 5170000 > 5180000 > 5190000 > 5200000 > 5210000 > 5220000 > 5230000 > 5240000 > 5250000 > 5260000 > 5270000 > 5280000 > 5290000 > 5300000 > 5310000 > 5320000 > 5330000 > 5340000 > 5350000 > 5360000 > 5370000 > 5380000 > 5390000 > 5400000 > 5410000 > 5420000 > 5430000 > 5440000 > 5450000 > 5460000 > 5470000 > 5480000 > 5490000 > 5500000 > 5510000 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.3437
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 17,
      "question": "how is the weather in jamaica",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455 > 455",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.51559
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 18,
      "question": "difference between a mcdouble and a double cheeseburger",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 > 8434621 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.70061
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 19,
      "question": "hydrogen is a liquid below what temperature",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128 > 128",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.54439
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 20,
      "question": "who is robert gray",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 8760873 > 8760873 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > 454 > ",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_16k_c4096",
      "example_index": 21,
      "question": "is cdg airport in main paris",
      "truncation_type": "repeated_same_id",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 4704975 > 4704975 > 4704975\n\n[ID: 7059831] Document: The main airport in the region is Minneapolis\u2013St. Paul International Airport (MSP), which is a major hub for Delta Air Lines. The airport is located about 10 miles (16 km) south of downtown Minneapolis and 16 miles (25 km) north of downtown St. Paul.\n\n[ID: 3194191] Document: The main airport in the region is Minneapolis\u2013St. Paul International Airport (MSP), which is a major hub for Delta Air Lines. The airport is located about 10 miles (16 km) south of downtown Minneapolis and 16 miles (25 km) north of downtown St. Paul.\n\n[ID: 3194191] Document: The main airport in the region is Minneapolis\u2013St. Paul International Airport (MSP), which is a major hub for Delta Air Lines. The airport is located about 10 miles (16 km)",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 0,
      "question": "definition of a sigmet",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8757181 > 8307181 > 8307180 > 8307182 > 8307183 > 8307184 > 8307185 > 8307186 > 8307187 > 8307188 > 8307189 > 8307190 > 8307191 > 8307192 > 8307193 > 8307194 > 8307195 > 8307196 > 8307197 > 8307198 > 8307199 > 8307200 > 8307201 > 8307202 > 8307203 > 8307204 > 8307205 > 8307206 > 8307207 > 8307208 > 8307209 > 8307210 > 8307211 > 8307212 > 8307213 > 8307214 > 8307215 > 8307216 > 8307217 > 8307218 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.80069
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 1,
      "question": "what types of food can you cook sous vide",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821 > 821",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.4926
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 2,
      "question": "causes of military suicide",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 528353 > 511111 > 500000 > 489999 > 479999 > 469999 > 459999 > 449999 > 439999 > 429999 > 419999 > 409999 > 399999 > 389999 > 379999 > 369999 > 359999 > 349999 > 339999 > 329999 > 319999 > 309999 > 299999 > 289999 > 279999 > 269999 > 259999 > 249999 > 239999 > 229999 > 219999 > 209999 > 199999 > 189999 > 179999 > 169999 > 159999 > 149999 > 139999 > 129999 > 119999 > 109999 > 99999 > 89999 > 79999 > 69999 > 59999 > 49999 > 39999 > 29999 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.09257
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 3,
      "question": "exons definition biology",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 6101181 > 6101182 > 6101183 > 6101184 > 6101185 > 6101186 > 6101187 > 6101188 > 6101189 > 6101190 > 6101191 > 6101192 > 6101193 > 6101194 > 6101195 > 6101196 > 6101197 > 6101198 > 6101199 > 6101200 > 6101201 > 6101202 > 6101203 > 6101204 > 6101205 > 6101206 > 6101207 > 6101208 > 6101209 > 6101210 > 6101211 > 6101212 > 6101213 > 6101214 > 6101215 > 6101216 > 6101217 > 6101218 > 6101219 > 6101220 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.64292
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 4,
      "question": "what is a active margin",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 528 > 528 > 528\n\n[ID: 456] Document: The term \"margin\" is used in various contexts, including finance, geography, and construction. In finance, a margin is the difference between the selling price and the cost of a security or commodity. In geography, a margin is a boundary or edge of a landmass, ocean, or other geographical feature. In construction, a margin is the space between the edge of a building or other structure and the property line.\n\nQuery: what is a margin\nRanking: 456 > 456 > 456\n\n[ID: 123] Document: The term \"margin\" is used in various contexts, including finance, geography, and construction. In finance, a margin is the difference between the selling price and the cost of a security or commodity. In geography, a margin is a boundary or edge of a landmass, ocean, or other geographical feature. In construction, a margin is the space between the edge of",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.42884
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 5,
      "question": "what is the most popular food in switzerland",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8760871 > 364 > 371 > 370 > 369 > 368 > 367 > 366 > 365 > 364 > 363 > 362 > 361 > 360 > 359 > 358 > 357 > 356 > 355 > 354 > 353 > 352 > 351 > 350 > 349 > 348 > 347 > 346 > 345 > 344 > 343 > 342 > 341 > 340 > 339 > 338 > 337 > 336 > 335 > 334 > 333 > 332 > 331 > 330 > 329 > 328 > 327 > 326 > 325 > 324 > 323 > 322 > 321 > 320 > 319 > 318 > 317 > 316 > 315 > 314 > 313 > 312 > 311 > 310 > 309 > 308 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.68423
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 6,
      "question": "what can contour plowing reduce",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 805 > 804 > 803 > 802 > 801 > 800 > 799 > 798 > 797 > 796 > 795 > 794 > 793 > 792 > 791 > 790 > 789 > 788 > 787 > 786 > 785 > 784 > 783 > 782 > 781 > 780 > 779 > 778 > 777 > 776 > 775 > 774 > 773 > 772 > 771 > 770 > 769 > 768 > 767 > 766 > 765 > 764 > 763 > 762 > 761 > 760 > 759 > 758 > 757 > 756 > 755 > 754 > 753 > 752 > 751 > 750 > 749 > 748 > 747 > 746 > 745 > 744 > 743 > 742 > 741 > 740 > 739",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.42493
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 7,
      "question": "who formed the commonwealth of independent states",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 275111 > 275110 > 275109 > 275108 > 275107 > 275106 > 275105 > 275104 > 275103 > 275102 > 275101 > 275100 > 275099 > 275098 > 275097 > 275096 > 275095 > 275094 > 275093 > 275092 > 275091 > 275090 > 275089 > 275088 > 275087 > 275086 > 275085 > 275084 > 275083 > 275082 > 275081 > 275080 > 275079 > 275078 > 275077 > 275076 > 275075 > 275074 > 275073 > 275072 > 275071 > 275070 > 275069 > 275068 > 275067 > 275066 > 275065 > 275064 > 275063 > 275062 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.312
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 8,
      "question": "when was the salvation army founded",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 878 > 232 > 232\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and Catherine Mumford Booth.\n\n[ID: 232: 0] Document: The Salvation Army was founded in 1865 by William Booth and",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.58847
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 9,
      "question": "rsa definition key",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 473111 > 484111 > 484112 > 484113 > 484114 > 484115 > 484116 > 484117 > 484118 > 484119 > 484120 > 484121 > 484122 > 484123 > 484124 > 484125 > 484126 > 484127 > 484128 > 484129 > 484130 > 484131 > 484132 > 484133 > 484134 > 484135 > 484136 > 484137 > 484138 > 484139 > 484140 > 484141 > 484142 > 484143 > 484144 > 484145 > 484146 > 484147 > 484148 > 484149 > 484150 > 484151 > 484152 > 484153 > 484154 > 484155 > 484156 > 484157 > 484158 > 484159 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.41775
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 10,
      "question": "tracheids are part of _____.",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 225111 > 225112 > 225113 > 225114 > 225115 > 225116 > 225117 > 225118 > 225119 > 225120 > 225121 > 225122 > 225123 > 225124 > 225125 > 225126 > 225127 > 225128 > 225129 > 225130 > 225131 > 225132 > 225133 > 225134 > 225135 > 225136 > 225137 > 225138 > 225139 > 225140 > 225141 > 225142 > 225143 > 225144 > 225145 > 225146 > 225147 > 225148 > 225149 > 225150 > 225151 > 225152 > 225153 > 225154 > 225155 > 225156 > 225157 > 225158 > 225159 > 225160 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.39484
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 11,
      "question": "how long is life cycle of flea",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 968 > 613 > 357 > 247 > 196 > 170 > 157 > 147 > 134 > 126 > 118 > 114 > 108 > 104 > 98 > 94 > 90 > 86 > 82 > 78 > 74 > 70 > 66 > 62 > 58 > 54 > 50 > 46 > 42 > 38 > 34 > 30 > 26 > 22 > 18 > 14 > 10 > 6 > 2\n\n[ID: 231] Document: The life cycle of a butterfly is a complex process that involves four stages: egg, larva, pupa, and adult. The female butterfly lays her eggs on a leaf or stem, and the eggs hatch into larvae after a few days. The larvae go through a series of molts as they grow, and eventually form a pupa. Inside the pupa, the adult",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.37586
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 12,
      "question": "what are the social determinants of health",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8444141 > 8444142 > 8444143 > 8444144 > 8444145 > 8444146 > 8444147 > 8444148 > 8444149 > 8444150 > 8444151 > 8444152 > 8444153 > 8444154 > 8444155 > 8444156 > 8444157 > 8444158 > 8444159 > 8444160 > 8444161 > 8444162 > 8444163 > 8444164 > 8444165 > 8444166 > 8444167 > 8444168 > 8444169 > 8444170 > 8444171 > 8444172 > 8444173 > 8444174 > 8444175 > 8444176 > 8444177 > 8444178 > 8444179 > 8444180 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.09914
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 13,
      "question": "axon terminals or synaptic knob definition",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8412682 > 8412681 > 8412680 > 8412679 > 8412678 > 8412677 > 8412676 > 8412675 > 8412674 > 8412673 > 8412672 > 8412671 > 8412670 > 8412669 > 8412668 > 8412667 > 8412666 > 8412665 > 8412664 > 8412663 > 8412662 > 8412661 > 8412660 > 8412659 > 8412658 > 8412657 > 8412656 > 8412655 > 8412654 > 8412653 > 8412652 > 8412651 > 8412650 > 8412649 > 8412648 > 8412647 > 8412646 > 8412645 > 8412644 > 8412643 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.15717
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 14,
      "question": "example of monotonic function",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 2911111 > 2911112 > 2911113\n\n[ID: 2911111] Document: Monotonic function. A function f is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function f is said to be monotonically increasing if for all x and y in the domain of f, if x < y, then f(x) < f(y). A function f is said to be monotonically decreasing if for all x and y in the domain of f, if x < y, then f(x) > f(y).\n\n[ID: 2911112] Document: Monotonic function. A function f is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function f is said to be monotonically increasing if for all x and y in the domain of f, if x < y, then f(x) < f(y). A function f",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.19905
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 15,
      "question": "causes of left ventricular hypertrophy",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833 > 833",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.3437
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 16,
      "question": "lps laws definition",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 322 > 322 > 322\n\n[ID: 322] Definition: LPS stands for Lipopolysaccharides, which are large molecules found in the outer membrane of Gram-negative bacteria. They play a crucial role in the immune response and are often used as a marker for bacterial infections.\n\n[ID: 322] Definition: LPS is an acronym for Lightning Protection System, which is a network of conductors and grounding systems designed to protect buildings and structures from lightning strikes.\n\n[ID: 322] Definition: LPS stands for Large Public Service, which is a term used to describe a government agency or department that provides essential services to the public.\n\n[ID: 322] Definition: LPS is an acronym for Local Public Safety, which refers to the efforts and initiatives taken by local governments to ensure the safety and well-being of their citizens.\n\n[ID: 322] Definition: LPS stands for Large Public Sector, which refers to the government sector that provides essential services to the public",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.5338
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 17,
      "question": "what is the daily life of thai people",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813 > 813",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.59586
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 18,
      "question": "how to find the midsegment of a trapezoid",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 576 > 818 > 743 > 222 > 576 > 743 > 818 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743 > 743",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.07336
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 19,
      "question": "what is wifi vs bluetooth",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190 > 190",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.52184
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 20,
      "question": "anthropological definition of environment",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 841 > 840 > 839 > 838 > 837 > 836 > 835 > 834 > 833 > 832 > 831 > 830 > 829 > 828 > 827 > 826 > 825 > 824 > 823 > 822 > 821 > 820 > 819 > 818 > 817 > 816 > 815 > 814 > 813 > 812 > 811 > 810 > 809 > 808 > 807 > 806 > 805 > 804 > 803 > 802 > 801 > 800 > 799 > 798 > 797 > 796 > 795 > 794 > 793 > 792 > 791 > 790 > 789 > 788 > 787 > 786 > 785 > 784 > 783 > 782 > 781 > 780 > 779 > 778 > 777 > 776 > 775",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.65284
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 21,
      "question": "define visceral?",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133 > 133",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.50709
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 22,
      "question": "does legionella pneumophila cause pneumonia",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130 > 130",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.62653
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 23,
      "question": "medicare's definition of mechanical ventilation",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8391111 > 5171111 > 5171112 > 5171113 > 5171114 > 5171115 > 5171116 > 5171117 > 5171118 > 5171119 > 5171120 > 5171121 > 5171122 > 5171123 > 5171124 > 5171125 > 5171126 > 5171127 > 5171128 > 5171129 > 5171130 > 5171131 > 5171132 > 5171133 > 5171134 > 5171135 > 5171136 > 5171137 > 5171138 > 5171139 > 5171140 > 5171141 > 5171142 > 5171143 > 5171144 > 5171145 > 5171146 > 5171147 > 5171148 > 5171149 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.28559
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 24,
      "question": "is cdg airport in main paris",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 702 > 756 > 758 > 760 > 762 > 764 > 766 > 768 > 770 > 772 > 774 > 776 > 778 > 780 > 782 > 784 > 786 > 788 > 790 > 792 > 794 > 796 > 798 > 800 > 802 > 804 > 806 > 808 > 810 > 812 > 814 > 816 > 818 > 820 > 822 > 824 > 826 > 828 > 830 > 832 > 834 > 836 > 838 > 840 > 842 > 844 > 846 > 848 > 850 > 852 > 854 > 856 > 858 > 860 > 862 > 864 > 866 > 868 > 870 > 872 > 874 > 876 > 878 > 880 > 882 > 884 > 886",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.17343
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 25,
      "question": "what is an aml surveillance analyst",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 4311111 > 4311112 > 4311113 > 4311114 > 4311115 > 4311116 > 4311117 > 4311118 > 4311119 > 4311120\n\nNote: The ranking is based on the relevance of the documents to the query \"what is an AML surveillance analyst\". The ranking is subjective and may vary based on individual perspectives. \n\nThe top-ranked document (4311111) is a detailed description of the role of an AML surveillance analyst, including their responsibilities, skills, and qualifications. The document provides a comprehensive overview of the position and its importance in the field of public health.\n\nThe second-ranked document (4311112) is a job description for an AML surveillance analyst, highlighting the key responsibilities and qualifications required for the position. The document provides a clear understanding of the role and its requirements.\n\nThe third-ranked document (4311113) is a discussion on the importance of AML",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.28024
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 26,
      "question": "how are some sharks warm blooded",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139 > 139",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.56379
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 27,
      "question": "why did the us volunterilay enter ww1",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122 > 122",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.53611
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 28,
      "question": "what is durable medical equipment consist of",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 131 > 132 > 133 > 134 > 135 > 136 > 137 > 138 > 139 > 140 > 141 > 142 > 143 > 144 > 145 > 146 > 147 > 148 > 149 > 150 > 151 > 152 > 153 > 154 > 155 > 156 > 157 > 158 > 159 > 160 > 161 > 162 > 163 > 164 > 165 > 166 > 167 > 168 > 169 > 170 > 171 > 172 > 173 > 174 > 175 > 176 > 177 > 178 > 179 > 180 > 181 > 182 > 183 > 184 > 185 > 186 > 187 > 188 > 189 > 190 > 191 > 192 > 193 > 194 > 195 > 196 > 197",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.26324
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 29,
      "question": "types of dysarthria from cerebral palsy",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 384 > 383 > 382 > 381 > 380 > 379 > 378 > 377 > 376 > 375 > 374 > 373 > 372 > 371 > 370 > 369 > 368 > 367 > 366 > 365 > 364 > 363 > 362 > 361 > 360 > 359 > 358 > 357 > 356 > 355 > 354 > 353 > 352 > 351 > 350 > 349 > 348 > 347 > 346 > 345 > 344 > 343 > 342 > 341 > 340 > 339 > 338 > 337 > 336 > 335 > 334 > 333 > 332 > 331 > 330 > 329 > 328 > 327 > 326 > 325 > 324 > 323 > 322 > 321 > 320 > 319 > 318",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.15275
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 30,
      "question": "cost of interior concrete flooring",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 513 > 544 > 543 > 542 > 541 > 540 > 539 > 538 > 537 > 536 > 535 > 534 > 533 > 532 > 531 > 530 > 529 > 528 > 527 > 526 > 525 > 524 > 523 > 522 > 521 > 520 > 519 > 518 > 517 > 516 > 515 > 514 > 513 > 512 > 511 > 510 > 509 > 508 > 507 > 506 > 505 > 504 > 503 > 502 > 501 > 500 > 499 > 498 > 497 > 496 > 495 > 494 > 493 > 492 > 491 > 490 > 489 > 488 > 487 > 486 > 485 > 484 > 483 > 482 > 481 > 480 > 479",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.80069
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 31,
      "question": "how is the weather in jamaica",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 709 > 707 > 706 > 705 > 704 > 703 > 702 > 701 > 700 > 699 > 698 > 697 > 696 > 695 > 694 > 693 > 692 > 691 > 690 > 689 > 688 > 687 > 686 > 685 > 684 > 683 > 682 > 681 > 680 > 679 > 678 > 677 > 676 > 675 > 674 > 673 > 672 > 671 > 670 > 669 > 668 > 667 > 666 > 665 > 664 > 663 > 662 > 661 > 660 > 659 > 658 > 657 > 656 > 655 > 654 > 653 > 652 > 651 > 650 > 649 > 648 > 647 > 646 > 645 > 644 > 643 > 642",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.31267
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 32,
      "question": "difference between a mcdouble and a double cheeseburger",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8433900 > 8433900 > 8433900\n\nThe ranking is not possible to determine as the documents are identical. \n\nHowever, based on the query, the answer is that the McDouble and the Double Cheeseburger are essentially the same thing, with the only difference being the amount of cheese. The McDouble has one slice of cheese, while the Double Cheeseburger has two slices. The rest of the ingredients, including the meat patties, pickles, onions, and condiments, are the same. \n\nTherefore, the correct answer is that there is no difference between a McDouble and a Double Cheeseburger, except for the amount of cheese. \n\nRanking: 8433900 > 8433900 > 8433900\n\nHowever, if we are to follow the format of the question, we can say that the Double Cheeseburger is more relevant to the query as it has more cheese, which is the main difference between the two burgers. \n\nRank",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.57652
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 33,
      "question": "do goldfish grow",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.84355
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 34,
      "question": "hydrogen is a liquid below what temperature",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 390 > 382 > 379 > 377 > 375 > 373 > 371 > 369 > 367 > 365 > 363 > 361 > 359 > 357 > 355 > 353 > 351 > 349 > 347 > 345 > 343 > 341 > 339 > 337 > 335 > 333 > 331 > 329 > 327 > 325 > 323 > 321 > 319 > 317 > 315 > 313 > 311 > 309 > 307 > 305 > 303 > 301 > 299 > 297 > 295 > 293 > 291 > 289 > 287 > 285 > 283 > 281 > 279 > 277 > 275 > 273 > 271 > 269 > 267 > 265 > 263 > 261 > 259 > 257 > 255 > 253 > 251",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.20512
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 35,
      "question": "definition declaratory judgment",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 861 > 861 > 861\n\nNote: The ranking is the same for all three documents, as they all have the same definition.\n\n[ID: 861] Definition: A declaratory judgment is a judicial pronouncement that defines the rights, duties, and obligations of the parties involved in a dispute. It is a type of judicial remedy that provides a legal declaration of the rights and obligations of the parties, without necessarily requiring them to take any further action.\n\n[ID: 861] Definition: A declaratory judgment is a court order that declares the rights and obligations of the parties involved in a dispute. It is a type of judicial remedy that provides a legal declaration of the rights and obligations of the parties, without necessarily requiring them to take any further action.\n\n[ID: 861] Definition: A declaratory judgment is a judicial pronouncement that defines the rights, duties, and obligations of the parties involved in a dispute. It is a type of judicial remedy that provides a legal declaration of the",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.62015
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 36,
      "question": "what is famvir prescribed for",
      "truncation_type": "sequential_numbers",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 4411111 > 4411112 > 4411113 > 4411114 > 4411115 > 4411116 > 4411117 > 4411118 > 4411119 > 4411120 > 4411121 > 4411122 > 4411123 > 4411124 > 4411125 > 4411126 > 4411127 > 4411128 > 4411129 > 4411130 > 4411131 > 4411132 > 4411133 > 4411134 > 4411135 > 4411136 > 4411137 > 4411138 > 4411139 > 4411140 > 4411141 > 4411142 > 4411143 > 4411144 > 4411145 > 4411146 > 4411147 > 4411148 > 4411149 > 4411150 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 37,
      "question": "who is robert gray",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 876 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822 > 822",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 38,
      "question": "right pelvic pain causes",
      "truncation_type": "sequential_numbers",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 1312841 > 1312842 > 1312843 > 1312844 > 1312845 > 1312846 > 1312847 > 1312848 > 1312849 > 1312850\n\nQuery: causes of pelvic pain\nRanking: 1312841 > 1312842 > 1312843 > 1312844 > 1312845 > 1312846 > 1312847 > 1312848 > 1312849 > 1312850\n\nQuery: causes of right pelvic pain\nRanking: 1312841 > 1312842 > 1312843 > 1312844 > 1312845 > 1312846 > 1312847 > 1312848 > 1312849 > 1312850\n\nQuery: causes of pelvic pain in women\nRanking: 1312841 > 1312842 > 1312843 > 131",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 39,
      "question": "what is physical description of spruce",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 462 > 554 > 555 > 556 > 557 > 558 > 559 > 560 > 561 > 562 > 563 > 564 > 565 > 566 > 567 > 568 > 569 > 570 > 571 > 572 > 573 > 574 > 575 > 576 > 577 > 578 > 579 > 580 > 581 > 582 > 583 > 584 > 585 > 586 > 587 > 588 > 589 > 590 > 591 > 592 > 593 > 594 > 595 > 596 > 597 > 598 > 599 > 600 > 601 > 602 > 603 > 604 > 605 > 606 > 607 > 608 > 609 > 610 > 611 > 612 > 613 > 614 > 615 > 616 > 617 > 618 > 619",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c1024",
      "example_index": 40,
      "question": "difference between rn and bsn",
      "truncation_type": "sequential_numbers",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 138 > 136 > 135 > 134 > 133 > 132 > 131 > 130 > 129 > 128 > 127 > 126 > 125 > 124 > 123 > 122 > 121 > 120 > 119 > 118 > 117 > 116 > 115 > 114 > 113 > 112 > 111 > 110 > 109 > 108 > 107 > 106 > 105 > 104 > 103 > 102 > 101 > 100 > 99 > 98 > 97 > 96 > 95 > 94 > 93 > 92 > 91 > 90 > 89 > 88 > 87 > 86 > 85 > 84 > 83 > 82 > 81 > 80 > 79 > 78 > 77 > 76 > 75 > 74 > 73 > 72 > 71",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 0,
      "question": "what types of food can you cook sous vide",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 821111 > 821112 > 821113 > 821114 > 821115 > 821116 > 821117 > 821118 > 821119 > 821120 > 821121 > 821122 > 821123 > 821124 > 821125 > 821126 > 821127 > 821128 > 821129 > 821130 > 821131 > 821132 > 821133 > 821134 > 821135 > 821136 > 821137 > 821138 > 821139 > 821140 > 821141 > 821142 > 821143 > 821144 > 821145 > 821146 > 821147 > 821148 > 821149 > 821150 > 821151 > 821152 > 821153 > 821154 > 821155 > 821156 > 821157 > 821158 > 821159 > 821160 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.4926
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 1,
      "question": "causes of military suicide",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 5251683 > 3501111 > 3501112 > 3501113 > 3501114 > 3501115 > 3501116 > 3501117 > 3501118 > 3501119 > 3501120 > 3501121 > 3501122 > 3501123 > 3501124 > 3501125 > 3501126 > 3501127 > 3501128 > 3501129 > 3501130 > 3501131 > 3501132 > 3501133 > 3501134 > 3501135 > 3501136 > 3501137 > 3501138 > 3501139 > 3501140 > 3501141 > 3501142 > 3501143 > 3501144 > 3501145 > 3501146 > 3501147 > 3501148 > 3501149 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.09257
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 2,
      "question": "exons definition biology",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8574921 > 4494921 > 4494922 > 4494923 > 4494924 > 4494925 > 4494926 > 4494927 > 4494928 > 4494929 > 4494930 > 4494931 > 4494932 > 4494933 > 4494934 > 4494935 > 4494936 > 4494937 > 4494938 > 4494939 > 4494940 > 4494941 > 4494942 > 4494943 > 4494944 > 4494945 > 4494946 > 4494947 > 4494948 > 4494949 > 4494950 > 4494951 > 4494952 > 4494953 > 4494954 > 4494955 > 4494956 > 4494957 > 4494958 > 4494959 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.64292
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 3,
      "question": "what is a active margin",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 525 > 528 > 527 > 526 > 525 > 524 > 523 > 522 > 521 > 520 > 519 > 518 > 517 > 516 > 515 > 514 > 513 > 512 > 511 > 510 > 509 > 508 > 507 > 506 > 505 > 504 > 503 > 502 > 501 > 500 > 499 > 498 > 497 > 496 > 495 > 494 > 493 > 492 > 491 > 490 > 489 > 488 > 487 > 486 > 485 > 484 > 483 > 482 > 481 > 480 > 479 > 478 > 477 > 476 > 475 > 474 > 473 > 472 > 471 > 470 > 469 > 468 > 467 > 466 > 465 > 464 > 463",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.42884
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 4,
      "question": "what is the most popular food in switzerland",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 > 238170 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.68423
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 5,
      "question": "what can contour plowing reduce",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805 > 805",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.42493
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 6,
      "question": "who formed the commonwealth of independent states",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8224175 > 8224189 > 8224188 > 8224187 > 8224186 > 8224185 > 8224184 > 8224183 > 8224182 > 8224181 > 8224180 > 8224179 > 8224178 > 8224177 > 8224176 > 8224174 > 8224173 > 8224172 > 8224171 > 8224170 > 8224169 > 8224168 > 8224167 > 8224166 > 8224165 > 8224164 > 8224163 > 8224162 > 8224161 > 8224160 > 8224159 > 8224158 > 8224157 > 8224156 > 8224155 > 8224154 > 8224153 > 8224152 > 8224151 > 8224150 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.312
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 7,
      "question": "rsa definition key",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 4731111 > 4731112 > 4731113 > 4731114 > 4731115 > 4731116 > 4731117 > 4731118 > 4731119 > 4731120 > 4731121 > 4731122 > 4731123 > 4731124 > 4731125 > 4731126 > 4731127 > 4731128 > 4731129 > 4731130 > 4731131 > 4731132 > 4731133 > 4731134 > 4731135 > 4731136 > 4731137 > 4731138 > 4731139 > 4731140 > 4731141 > 4731142 > 4731143 > 4731144 > 4731145 > 4731146 > 4731147 > 4731148 > 4731149 > 4731150 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.41775
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 8,
      "question": "tracheids are part of _____.",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744 > 744",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.39484
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 9,
      "question": "what are the social determinants of health",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8444643 > 8444644 > 8444645 > 8444646 > 8444647 > 8444648 > 8444649 > 8444650 > 8444651 > 8444652 > 8444653 > 8444654 > 8444655 > 8444656 > 8444657 > 8444658 > 8444659 > 8444660 > 8444661 > 8444662 > 8444663 > 8444664 > 8444665 > 8444666 > 8444667 > 8444668 > 8444669 > 8444670 > 8444671 > 8444672 > 8444673 > 8444674 > 8444675 > 8444676 > 8444677 > 8444678 > 8444679 > 8444680 > 8444681 > 8444682 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.09914
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 10,
      "question": "example of monotonic function",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 2916091 > 2916092 > 2916093\n\n[ID: 2916091] Document: A function is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function is monotonically increasing if for all x1 and x2 in the domain of the function, if x1 < x2, then f(x1) < f(x2). A function is monotonically decreasing if for all x1 and x2 in the domain of the function, if x1 < x2, then f(x1) > f(x2).\n\n[ID: 2916092] Document: A function is said to be monotonic if it is either monotonically increasing or monotonically decreasing. A function is monotonically increasing if for all x1 and x2 in the domain of the function, if x1 < x2, then f(x1) < f(x2). A function is monotonically",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.19905
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 12,
      "question": "lps laws definition",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 3222561 > 3222562 > 3222563 > 3222564 > 3222565 > 3222566 > 3222567 > 3222568 > 3222569 > 3222570 > 3222571 > 3222572 > 3222573 > 3222574 > 3222575 > 3222576 > 3222577 > 3222578 > 3222579 > 3222580 > 3222581 > 3222582 > 3222583 > 3222584 > 3222585 > 3222586 > 3222587 > 3222588 > 3222589 > 3222590 > 3222591 > 3222592 > 3222593 > 3222594 > 3222595 > 3222596 > 3222597 > 3222598 > 3222599 > 3222600 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.5338
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 13,
      "question": "how to find the midsegment of a trapezoid",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561 > 561",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.07336
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 14,
      "question": "what is wifi vs bluetooth",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 190187 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > ",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.52184
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 15,
      "question": "anthropological definition of environment",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 841 > 443 > 342 > 234 > 123 > 56 > 34 > 17 > 8 > 4 > 2 > 1\n\nNote: The ranking is based on the number of times each document appears in the list of results, with the most frequent document appearing first. The ranking is not necessarily indicative of the relevance or accuracy of the documents. \n\nThe final answer is: Ranking: 841 > 443 > 342 > 234 > 123 > 56 > 34 > 17 > 8 > 4 > 2 > 1.",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.65284
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 16,
      "question": "define visceral?",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 > 133333 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.50709
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 17,
      "question": "does legionella pneumophila cause pneumonia",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 130130 > 130131 > 130132 > 130133 > 130134 > 130135 > 130136 > 130137 > 130138 > 130139 > 130140 > 130141 > 130142 > 130143 > 130144 > 130145 > 130146 > 130147 > 130148 > 130149 > 130150 > 130151 > 130152 > 130153 > 130154 > 130155 > 130156 > 130157 > 130158 > 130159 > 130160 > 130161 > 130162 > 130163 > 130164 > 130165 > 130166 > 130167 > 130168 > 130169 > 130170 > 130171 > 130172 > 130173 > 130174 > 130175 > 130176 > 130177 > 130178 > 130179 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.62653
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 18,
      "question": "medicare's definition of mechanical ventilation",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8391111 > 8391112 > 8391113 > 8391114 > 8391115 > 8391116 > 8391117 > 8391118 > 8391119 > 8391120 > 8391121 > 8391122 > 8391123 > 8391124 > 8391125 > 8391126 > 8391127 > 8391128 > 8391129 > 8391130 > 8391131 > 8391132 > 8391133 > 8391134 > 8391135 > 8391136 > 8391137 > 8391138 > 8391139 > 8391140 > 8391141 > 8391142 > 8391143 > 8391144 > 8391145 > 8391146 > 8391147 > 8391148 > 8391149 > 8391150 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.28559
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 19,
      "question": "is cdg airport in main paris",
      "truncation_type": "suspiciously_short_ids",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 772 > 756 > 748 > 744 > 742 > 740 > 738 > 736 > 734 > 732 > 730 > 728 > 726 > 724 > 722 > 720 > 718 > 716 > 714 > 712 > 710 > 708 > 706 > 704 > 702 > 700 > 698 > 696 > 694 > 692 > 690 > 688 > 686 > 684 > 682 > 680 > 678 > 676 > 674 > 672 > 670 > 668 > 666 > 664 > 662 > 660 > 658 > 656 > 654 > 652 > 650 > 648 > 646 > 644 > 642 > 640 > 638 > 636 > 634 > 632 > 630 > 628 > 626 > 624 > 622 > 620 > 618",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.17343
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 20,
      "question": "what is an aml surveillance analyst",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8100000 > 8100001 > 8100002\n\n[ID: 8100000] Document: AML Surveillance Analyst. The AML Surveillance Analyst is responsible for monitoring and analyzing data to identify potential money laundering activity. This includes reviewing customer information, transaction data, and other relevant information to identify suspicious activity.\n\n[ID: 8100001] Document: AML Surveillance Analyst Job Description. The AML Surveillance Analyst is responsible for monitoring and analyzing data to identify potential money laundering activity. This includes reviewing customer information, transaction data, and other relevant information to identify suspicious activity.\n\n[ID: 8100002] Document: AML Surveillance Analyst Salary. The average salary for an AML Surveillance Analyst is around $60,000 per year. However, salaries can range from $40,000 to over $100,000 depending on experience and location.\n\nQuery: what is aml surveillance\nRanking: 8100000 > 8100001 > 8100002\n\n",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.28024
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 21,
      "question": "how are some sharks warm blooded",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 139455 > 827 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > 751 > ",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.56379
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 22,
      "question": "what is durable medical equipment consist of",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 > 172111 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.26324
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 24,
      "question": "cost of interior concrete flooring",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 5131111 > 5131112 > 5131113 > 5131114 > 5131115 > 5131116 > 5131117 > 5131118 > 5131119 > 5131120 > 5131121 > 5131122 > 5131123 > 5131124 > 5131125 > 5131126 > 5131127 > 5131128 > 5131129 > 5131130 > 5131131 > 5131132 > 5131133 > 5131134 > 5131135 > 5131136 > 5131137 > 5131138 > 5131139 > 5131140 > 5131141 > 5131142 > 5131143 > 5131144 > 5131145 > 5131146 > 5131147 > 5131148 > 5131149 > 5131150 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.80069
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 25,
      "question": "how is the weather in jamaica",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 434 > 435 > 436 > 437 > 438 > 439 > 440 > 441 > 442 > 443 > 444 > 445 > 446 > 447 > 448 > 449 > 450 > 451 > 452 > 453 > 454 > 455 > 456 > 457 > 458 > 459 > 460 > 461 > 462 > 463 > 464 > 465 > 466 > 467 > 468 > 469 > 470 > 471 > 472 > 473 > 474 > 475 > 476 > 477 > 478 > 479 > 480 > 481 > 482 > 483 > 484 > 485 > 486 > 487 > 488 > 489 > 490 > 491 > 492 > 493 > 494 > 495 > 496 > 497 > 498 > 499 > 500",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.31267
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 26,
      "question": "do goldfish grow",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127 > 127",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.84355
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 28,
      "question": "definition declaratory judgment",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 861290 > 861291 > 861292\n\n[ID: 861290] Document: A declaratory judgment is a judgment that declares the rights, duties, and obligations of the parties to a lawsuit. It is a type of judgment that is used to resolve disputes and clarify the legal rights and obligations of the parties.\n\n[ID: 861291] Document: A declaratory judgment is a judgment that declares the rights, duties, and obligations of the parties to a lawsuit. It is a type of judgment that is used to resolve disputes and clarify the legal rights and obligations of the parties.\n\n[ID: 861292] Document: A declaratory judgment is a judgment that declares the rights, duties, and obligations of the parties to a lawsuit. It is a type of judgment that is used to resolve disputes and clarify the legal rights and obligations of the parties.\n\nQuery: what is the definition of declaratory judgment\nRanking: 861290 > 861291 > 861292\n\n",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.62015
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 29,
      "question": "what is famvir prescribed for",
      "truncation_type": "sequential_numbers",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 444444 > 444445 > 444446 > 444447 > 444448 > 444449 > 444450 > 444451 > 444452 > 444453 > 444454 > 444455 > 444456 > 444457 > 444458 > 444459 > 444460 > 444461 > 444462 > 444463 > 444464 > 444465 > 444466 > 444467 > 444468 > 444469 > 444470 > 444471 > 444472 > 444473 > 444474 > 444475 > 444476 > 444477 > 444478 > 444479 > 444480 > 444481 > 444482 > 444483 > 444484 > 444485 > 444486 > 444487 > 444488 > 444489 > 444490 > 444491 > 444492 > 444493 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 30,
      "question": "who is robert gray",
      "truncation_type": "incomplete_ranking",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 8760869 > 8224670 > 8224671 > 8224672 > 8224673 > 8224674 > 8224675 > 8224676 > 8224677 > 8224678 > 8224679 > 8224680 > 8224681 > 8224682 > 8224683 > 8224684 > 8224685 > 8224686 > 8224687 > 8224688 > 8224689 > 8224690 > 8224691 > 8224692 > 8224693 > 8224694 > 8224695 > 8224696 > 8224697 > 8224698 > 8224699 > 8224700 > 8224701 > 8224702 > 8224703 > 8224704 > 8224705 > 8224706 > 8224707 > 8224708 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 31,
      "question": "right pelvic pain causes",
      "truncation_type": "sequential_numbers",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 1411571 > 1411572 > 1411573 > 1411574 > 1411575 > 1411576 > 1411577 > 1411578 > 1411579 > 1411580 > 1411581 > 1411582 > 1411583 > 1411584 > 1411585 > 1411586 > 1411587 > 1411588 > 1411589 > 1411590 > 1411591 > 1411592 > 1411593 > 1411594 > 1411595 > 1411596 > 1411597 > 1411598 > 1411599 > 1411600 > 1411601 > 1411602 > 1411603 > 1411604 > 1411605 > 1411606 > 1411607 > 1411608 > 1411609 > 1411610 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 32,
      "question": "what is physical description of spruce",
      "truncation_type": "repeated_same_id",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554 > 554",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "llama_32k_c4096",
      "example_index": 33,
      "question": "difference between rn and bsn",
      "truncation_type": "sequential_numbers",
      "failure_category": "both_wrong",
      "snapkv_pred": "Ranking: 335330 > 335331 > 335332 > 335333 > 335334 > 335335 > 335336 > 335337 > 335338 > 335339 > 335340 > 335341 > 335342 > 335343 > 335344 > 335345 > 335346 > 335347 > 335348 > 335349 > 335350 > 335351 > 335352 > 335353 > 335354 > 335355 > 335356 > 335357 > 335358 > 335359 > 335360 > 335361 > 335362 > 335363 > 335364 > 335365 > 335366 > 335367 > 335368 > 335369 > 335370 > 335371 > 335372 > 335373 > 335374 > 335375 > 335376 > 335377 > 335378 > 335379 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.0
    },
    {
      "config": "qwen_16k_c1024",
      "example_index": 1,
      "question": "what is famvir prescribed for",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 818273 > 765432 > 654321 > 543210 > 432109 > 321098 > 210987 > 109876 > 987654 > 876543 > 765432 > 654321 > 543210 > 432109 > 321098 > 210987 > 109876 > 987654 > 876543 > 765432 > 654321 > 543210 > 432109 > 321098 > 210987 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.46435
    },
    {
      "config": "qwen_16k_c1024",
      "example_index": 8,
      "question": "who formed the commonwealth of independent states",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8359 > 20214 > 4045 > 3144 > 18525 > 18524 > 18523 > 18522 > 18521 > 18520 > 18519 > 18518 > 18517 > 18516 > 18515 > 18514 > 18513 > 18512 > 18511 > 18510 > 18509 > 18508 > 18507 > 18506 > 18505 > 18504 > 18503 > 18502 > 18501 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.34421
    },
    {
      "config": "qwen_16k_c1024",
      "example_index": 11,
      "question": "when was the salvation army founded",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 23294 > 23293 > 23292 > 23291 > 23290 > 23289 > 23288 > 23287 > 23286 > 23285\n\nNote: The numbers in the ranking represent the unique identifiers of the documents in the given dataset, and the order indicates the relevance to the query \"when was the salvation army founded\". The higher the number, the less relevant the document is to the query. However, since the actual IDs are not provided in the prompt, I've used a placeholder format (23294, 23293, etc.) to illustrate the ranking. In a real scenario, you would replace these placeholders with the actual IDs from the dataset. \n\nFor example, the correct ranking might look like this based on the content:\n\nRanking: 2",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.69314
    },
    {
      "config": "qwen_16k_c1024",
      "example_index": 23,
      "question": "anthropological definition of environment",
      "truncation_type": "repeated_same_id",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890 > 1352467890\n\nNote: All provided IDs have the same content regarding the definition of \"environment\" from an anthropological perspective, hence they are ranked equally. \n\nRanking: 1352467890 > 1352467890 > 1352467890 > 1352467890 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.12827
    },
    {
      "config": "qwen_16k_c1024",
      "example_index": 36,
      "question": "hydrogen is a liquid below what temperature",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 6674235 > 6674234 > 6674233 > 6674232 > 6674231 > 6674230 > 6674229 > 6674228 > 6674227 > 6674226\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to actual document IDs. The ranking is based on the relevance of the content to the query \"hydrogen is a liquid below what temperature\". The top-ranked document should contain the most accurate and direct information regarding the temperature at which hydrogen becomes a liquid. \n\nGiven the context, the correct answer to the query \"hydrogen is a liquid below what temperature\" is -252.87\u00b0C or 20.3 K, which is equivalent to -423",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.09479
    },
    {
      "config": "qwen_16k_c1024",
      "example_index": 37,
      "question": "definition declaratory judgment",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 14 > 13 > 12 > 11 > 10 > 9 > 8 > 7 > 6 > 5 > 4 > 3 > 2 > 1\n1. [ID: 862212]: 14\n2. [ID: 862211]: 13\n3. [ID: 862210]: 12\n4. [ID: 862209]: 11\n5. [ID: 862208]: 10\n6. [ID: 862207]: 9\n7. [ID: 862206]: 8\n8. [ID: 862205]: 7\n9. [ID: 862204]: 6\n10. [",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.90539
    },
    {
      "config": "qwen_16k_c4096",
      "example_index": 15,
      "question": "what are the social determinants of health",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 3843267 > 3843268 > 3843269 > 3843270 > 3843271 > 3843272 > 3843273 > 3843274 > 3843275 > 3843276\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to actual document IDs in the given list. The ranking is based on the content and relevance of the documents to the query \"what are the social determinants of health\". The actual IDs should be replaced with the correct ones from the provided list. Ranking: 3843267 > 3843268 > 3843269 > 3843270 > 3843271 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.14794
    },
    {
      "config": "qwen_16k_c4096",
      "example_index": 31,
      "question": "types of dysarthria from cerebral palsy",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8356792 > 8356791 > 8356790 > 8356789 > 8356788 > 8356787 > 8356786 > 8356785 > 8356784 > 8356783\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to any actual document IDs in the given list. The ranking is based on the content and relevance of the documents to the query \"types of dysarthria from cerebral palsy\". The actual IDs should be replaced with the correct ones from the provided list. Ranking: 8434567 > 8434568 > 8434569 > 8434570 > 843457",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.59314
    },
    {
      "config": "qwen_16k_c4096",
      "example_index": 34,
      "question": "difference between a mcdouble and a double cheeseburger",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 8432106 > 8432105 > 8432104 > 8432103 > 8432102 > 8432101 > 8432099 > 8432098 > 8432097 > 8432096\n\nNote: The IDs provided in the ranking are placeholders and do not correspond to any actual document IDs in the given list. The ranking is based on the relevance of the content to the query \"difference between a mcdouble and a double cheeseburger\". The actual IDs from the provided list should be used in the final answer. Ranking: 8432106 > 8432105 > 8432104 > 8432103 > 8432102",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.85663
    },
    {
      "config": "qwen_16k_c4096",
      "example_index": 35,
      "question": "do goldfish grow",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 2598764 > 2598763 > 2598762 > 2598761 > 2598760 > 2598759 > 2598758 > 2598757 > 2598756 > 2598755\n\nNote: The numbers in the ranking represent placeholders for the unique IDs of the documents, as the actual IDs are not provided in the given text. The ranking is based on the relevance of the content to the query \"do goldfish grow\". The higher the number, the less relevant the document is to the query. \n\nFor example, the ranking starts with the most relevant document (ID 2598764) and ends with the least relevant document (ID 2598755). The exact IDs are not provided",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.1251
    },
    {
      "config": "qwen_32k_c1024",
      "example_index": 4,
      "question": "who is robert gray",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 712586 > 178732 > 816254 > 332156 > 245879 > 654987 > 987654 > 543210 > 432109 > 321098 > 210987 > 109876 > 987651 > 876543 > 765432 > 654321 > 543219 > 432108 > 321097 > 210986 > 109875 > 987650 > 876542 > 765431 > 654320 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.1303
    },
    {
      "config": "qwen_32k_c1024",
      "example_index": 17,
      "question": "what is the daily life of thai people",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324875 > 1269324",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.56539
    },
    {
      "config": "qwen_32k_c1024",
      "example_index": 20,
      "question": "does legionella pneumophila cause pneumonia",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 1901823 > 1901822 > 1901821 > 1901820 > 1901819 > 1901818 > 1901817 > 1901816 > 1901815 > 1901814 > 1901813 > 1901812 > 1901811 > 1901810 > 1901809 > 1901808 > 1901807 > 1901806 > 1901805 > 1901804\n\n[ID: 1901823] Document: Legionella pneumophila is the",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.67259
    },
    {
      "config": "qwen_32k_c1024",
      "example_index": 21,
      "question": "medicare's definition of mechanical ventilation",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 12856347 > 6112356 > 8394567 > 2345678 > 9876543 > 5678901 > 4567890 > 3456789 > 7890123 > 6789012 > 5678902 > 4567891 > 3456788 > 2345679 > 1234567 > 8901234 > 7654321 > 6543210 > 5432109 > 4321098 > 3210987 > 2109876 > ",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.3824
    },
    {
      "config": "qwen_32k_c1024",
      "example_index": 23,
      "question": "what is an aml surveillance analyst",
      "truncation_type": "incomplete_ranking",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 81234567 > 12345678 > 23456789 > 34567890 > 45678901 > 56789012 > 67890123 > 78901234 > 89012345 > 90123456 > 01234567 > 12345678 > 23456789 > 34567890 > 45678901 > 56789012 > 67890123 > 78901234 > 89012345 > 90123456 >",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.12753
    },
    {
      "config": "qwen_32k_c4096",
      "example_index": 20,
      "question": "does legionella pneumophila cause pneumonia",
      "truncation_type": "sequential_numbers",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 1901827 > 1901826 > 1901825 > 1901824 > 1901823 > 1901822 > 1901821 > 1901820 > 1901819 > 1901818\n\n[ID: 1901827] Document: Legionnaires' disease is a severe and sometimes fatal type of pneumonia caused by the bacterium Legionella pneumophila. The bacteria are commonly found in natural water sources and can thrive in man-made water systems, such as cooling towers, hot tubs, and plumbing systems.\n\n[ID: 1901826] Document: Legionella pneumophila, a type of bacteria, is the primary cause of Legionnaires' disease, a severe form of pneumonia. The bacteria",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.67259
    },
    {
      "config": "qwen_32k_c4096",
      "example_index": 31,
      "question": "hydrogen is a liquid below what temperature",
      "truncation_type": "mostly_same_with_variations",
      "failure_category": "baseline_wins",
      "snapkv_pred": "Ranking: 3998642 > 8696811 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642 > 3998642\n\n[ID: 3998642] Document: Hydrogen is a colorless, odorless, tasteless, nonmetallic, chemical element with the symbol H and atomic number 1. With an atomic weight of 1.00794 u, hydrogen is the lightest and most abundant chemical element, constituting roughly 75% of the Universe's baryonic mass. Hydrogen is the most abundant element in the Universe and is a major component of stars in which it is a raw material for nuclear",
      "snapkv_ndcg": 0.0,
      "baseline_ndcg": 0.13886
    }
  ]
}